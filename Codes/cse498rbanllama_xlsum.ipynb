{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9548160,"sourceType":"datasetVersion","datasetId":5817419},{"sourceId":9676030,"sourceType":"datasetVersion","datasetId":5913714}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom datasets import load_dataset\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom transformers import MT5ForConditionalGeneration, AutoTokenizer ,DataCollatorForSeq2Seq, Trainer, TrainingArguments\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:05.571930Z","iopub.execute_input":"2025-01-28T15:22:05.572248Z","iopub.status.idle":"2025-01-28T15:22:11.885307Z","shell.execute_reply.started":"2025-01-28T15:22:05.572225Z","shell.execute_reply":"2025-01-28T15:22:11.884612Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install peft --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:11.892093Z","iopub.execute_input":"2025-01-28T15:22:11.892335Z","iopub.status.idle":"2025-01-28T15:22:15.001235Z","shell.execute_reply.started":"2025-01-28T15:22:11.892304Z","shell.execute_reply":"2025-01-28T15:22:15.000353Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:15.004375Z","iopub.execute_input":"2025-01-28T15:22:15.004579Z","iopub.status.idle":"2025-01-28T15:22:15.027248Z","shell.execute_reply.started":"2025-01-28T15:22:15.004560Z","shell.execute_reply":"2025-01-28T15:22:15.026360Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Remove nrows = ? for ","metadata":{}},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/cleaned-news-dataset/Cleaned_news_dataset.csv', usecols=['summary', 'article'], nrows=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:15.028170Z","iopub.execute_input":"2025-01-28T15:22:15.028496Z","iopub.status.idle":"2025-01-28T15:22:15.043450Z","shell.execute_reply.started":"2025-01-28T15:22:15.028464Z","shell.execute_reply":"2025-01-28T15:22:15.042553Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/cleaned-news-dataset/Cleaned_news_dataset.csv', usecols=['summary', 'article'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:15.044270Z","iopub.execute_input":"2025-01-28T15:22:15.044570Z","iopub.status.idle":"2025-01-28T15:22:15.057126Z","shell.execute_reply.started":"2025-01-28T15:22:15.044540Z","shell.execute_reply":"2025-01-28T15:22:15.056351Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\n\n# Load the Bengali portion of the XL-Sum dataset\ndataset = load_dataset('csebuetnlp/xlsum', 'bengali')\n\n# Combine all splits (train, validation, test) into one\ncombined_data = pd.concat(\n    [\n        pd.DataFrame(dataset[split])\n        for split in dataset.keys()\n    ],\n    ignore_index=True\n)\n\n# Keep only 'text' (article) and 'summary' columns\ncombined_data = combined_data[['text', 'summary']].rename(columns={'text': 'article'})\n\n# Display the first few rows\nprint(combined_data.head())\ndf=combined_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:15.060356Z","iopub.execute_input":"2025-01-28T15:22:15.060557Z","iopub.status.idle":"2025-01-28T15:22:19.051911Z","shell.execute_reply.started":"2025-01-28T15:22:15.060539Z","shell.execute_reply":"2025-01-28T15:22:19.051124Z"}},"outputs":[{"name":"stdout","text":"                                             article  \\\n0  দুর্গাপুজায় মহিষাসুর বধ্যে মধ্য দিয়ে অশুভর ও...   \n1  গত ২০ বছরে তিনি রাশিয়ার প্রেসিডেন্ট এবং প্রধা...   \n2  বরিশাল সদরে চলছে নির্বাচনী প্রচার প্রচারণা। যদ...   \n3  রবীন্দ্রনাথ ঠাকুর রবীন্দ্রনাথ ঠাকুর বাঙালির কা...   \n4  আর কুমিল্লা থেকে বাস ভাড়া করে অনেকের সাথে নিজ...   \n\n                                             summary  \n0  হিন্দু বাঙালীরা যে সময়ে তাদের সবথেকে বড় উৎসব...  \n1  ভ্লাদিমির পুতিন তাঁর ক্ষমতায় থাকার ২০ বছর পূর...  \n2  বাংলাদেশের দক্ষিণাঞ্চলীয় জেলা বরিশাল এখন তুমু...  \n3  দু'হাজার চার সালে বিবিসি বাংলা একটি 'শ্রোতা জর...  \n4  'ভাষণ শুরু আগে মাথার উপর দিয়ে বিমান আর হেলিকপ...  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.30, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:19.053373Z","iopub.execute_input":"2025-01-28T15:22:19.053594Z","iopub.status.idle":"2025-01-28T15:22:19.062917Z","shell.execute_reply.started":"2025-01-28T15:22:19.053575Z","shell.execute_reply":"2025-01-28T15:22:19.062359Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from datasets import Dataset\nds_train = Dataset.from_pandas(df_train)\nds_test = Dataset.from_pandas(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:19.063926Z","iopub.execute_input":"2025-01-28T15:22:19.064261Z","iopub.status.idle":"2025-01-28T15:22:20.207955Z","shell.execute_reply.started":"2025-01-28T15:22:19.064228Z","shell.execute_reply":"2025-01-28T15:22:20.207014Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Model on Alpaca-orca dataset","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-bangla-alpaca-orca-instruct-v0.0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-bangla-alpaca-orca-instruct-v0.0.1\")\n\n# Configure LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,   # Task type\n    inference_mode=False,           # Enable training mode\n    r=16,                           # LoRA rank\n    lora_alpha=32,                  # Scaling factor\n    lora_dropout=0.1                # Dropout rate\n)\n\n# Wrap the model with LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:22:20.208896Z","iopub.execute_input":"2025-01-28T15:22:20.209213Z","iopub.status.idle":"2025-01-28T15:24:38.527367Z","shell.execute_reply.started":"2025-01-28T15:22:20.209181Z","shell.execute_reply":"2025-01-28T15:24:38.526573Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c292d3c3f844cc8a0d7efcf509487e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5adf9568e9a742d9ae94dae1f4a1ab77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14911611c3d44dc9af06d4fea00f1fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/917 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cbcd12cf0904cc59f0ebb01606b7983"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0032063a284b3cae1b226fbc3a2889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4a9053d48b4f06892a5f8ba460f1b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f083a304edc4e2dbd2cd6bd9dd98813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b60ab679aaff4d5da560d400ed3225d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a98f0950d06247bf8fa1c3ba9d4be3b2"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): ModulesToSaveWrapper(\n          (original_module): Embedding(128256, 2048)\n          (modules_to_save): ModuleDict(\n            (default): Embedding(128256, 2048)\n          )\n        )\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=2048, out_features=128256, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=2048, out_features=128256, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Model on culturax dataset","metadata":{}},{"cell_type":"code","source":"# from peft import get_peft_model, LoraConfig, TaskType\n# from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# # Load the model and tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-unolp-culturax-base-v0.0.1\")\n# model = AutoModelForCausalLM.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-unolp-culturax-base-v0.0.1\")\n\n# # Configure LoRA\n# lora_config = LoraConfig(\n#     task_type=TaskType.CAUSAL_LM,   # Task type\n#     inference_mode=False,           # Enable training mode\n#     r=16,                           # LoRA rank\n#     lora_alpha=32,                  # Scaling factor\n#     lora_dropout=0.1                # Dropout rate\n# )\n\n# # Wrap the model with LoRA\n# model = get_peft_model(model, lora_config)\n# model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:24:38.528196Z","iopub.execute_input":"2025-01-28T15:24:38.528420Z","iopub.status.idle":"2025-01-28T15:24:38.531936Z","shell.execute_reply.started":"2025-01-28T15:24:38.528400Z","shell.execute_reply":"2025-01-28T15:24:38.531054Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Tokenize dataset with your model tokenizer","metadata":{}},{"cell_type":"code","source":"# Tokenize the data\ndef preprocess_function(examples):\n    inputs = tokenizer(\n        examples[\"article\"], max_length=256, truncation=True, padding=\"max_length\"\n    )\n    labels = tokenizer(\n        examples[\"summary\"], max_length=256, truncation=True, padding=\"max_length\"\n    )\n\n    # Replace pad token ID with -100 in labels (ignored during loss computation)\n    labels[\"input_ids\"] = [\n        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n        for label in labels[\"input_ids\"]\n    ]\n\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": labels[\"input_ids\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:24:38.532817Z","iopub.execute_input":"2025-01-28T15:24:38.533093Z","iopub.status.idle":"2025-01-28T15:24:38.552952Z","shell.execute_reply.started":"2025-01-28T15:24:38.533069Z","shell.execute_reply":"2025-01-28T15:24:38.552138Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n\n\n\ntokenized_train = ds_train.map(\n    preprocess_function, batched=True, remove_columns=[\"summary\", \"article\", \"__index_level_0__\"]\n)\ntokenized_test = ds_test.map(\n    preprocess_function, batched=True, remove_columns=[\"summary\", \"article\", \"__index_level_0__\"]\n)\n\n\n\n# Define a data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"longest\",\n    return_tensors=\"pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:24:38.553802Z","iopub.execute_input":"2025-01-28T15:24:38.554110Z","iopub.status.idle":"2025-01-28T15:25:14.000752Z","shell.execute_reply.started":"2025-01-28T15:24:38.554042Z","shell.execute_reply":"2025-01-28T15:25:14.000012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7088 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09f0dd0acd3e473cb810c7a921934486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3038 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90263ad29d847749f29772b2e6c89f7"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Debug stuff","metadata":{}},{"cell_type":"code","source":"print(tokenized_train[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:14.001519Z","iopub.execute_input":"2025-01-28T15:25:14.001759Z","iopub.status.idle":"2025-01-28T15:25:14.006650Z","shell.execute_reply.started":"2025-01-28T15:25:14.001737Z","shell.execute_reply":"2025-01-28T15:25:14.005912Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [128000, 11372, 105, 50228, 224, 11372, 110, 50228, 99, 60008, 11372, 114, 60008, 36278, 116, 11372, 105, 11372, 248, 60008, 11372, 107, 11372, 120, 60008, 36278, 250, 87648, 11372, 103, 53906, 108, 81278, 107, 11372, 120, 36278, 116, 50228, 106, 50228, 250, 81278, 243, 36278, 106, 50228, 100, 53906, 107, 11372, 106, 36278, 117, 11372, 110, 36278, 104, 60008, 11372, 116, 11372, 105, 28025, 223, 11372, 243, 100278, 36278, 116, 11372, 106, 53906, 103, 53906, 108, 11372, 97, 62456, 36278, 107, 28025, 223, 11372, 243, 53906, 97, 73358, 50228, 115, 53906, 253, 53906, 108, 60008, 73358, 36278, 101, 81278, 231, 11372, 229, 11372, 107, 11372, 120, 73358, 53906, 243, 60008, 36278, 116, 28025, 224, 73358, 53906, 107, 11372, 106, 28025, 223, 11372, 244, 28025, 222, 36278, 104, 28025, 223, 11372, 110, 60008, 73358, 36278, 237, 11372, 243, 36278, 105, 50228, 245, 50228, 101, 60008, 36278, 103, 73358, 81278, 105, 50228, 108, 36278, 101, 81278, 107, 11372, 120, 60008, 36278, 246, 28025, 223, 73358, 11372, 97, 60008, 36278, 245, 81278, 107, 11372, 120, 60008, 11372, 249, 81278, 110, 60008, 87648, 36278, 105, 50228, 224, 11372, 110, 50228, 99, 60008, 11372, 114, 60008, 73358, 36278, 243, 53906, 108, 81278, 243, 60008, 11372, 253, 36278, 97, 50228, 108, 11372, 243, 42412, 36278, 116, 50228, 243, 81278, 105, 36278, 228, 11372, 110, 36278, 117, 50228, 116, 50228, 101, 100278, 36278, 116, 60008, 11372, 244, 50228, 101, 60008, 36278, 97, 50228, 108, 36278, 114, 81278, 114, 28025, 223, 11372, 243, 87648, 53906, 107, 50228, 108, 36278, 243, 11372, 107, 11372, 120], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [128000, 11372, 116, 50228, 106, 50228, 250, 81278, 243, 36278, 107, 28025, 233, 11372, 245, 50228, 107, 28025, 233, 11372, 245, 36278, 106, 50228, 100, 53906, 107, 11372, 106, 60008, 36278, 116, 50228, 243, 81278, 105, 36278, 228, 11372, 110, 36278, 117, 50228, 116, 50228, 101, 60008, 73358, 36278, 114, 81278, 114, 28025, 223, 11372, 243, 87648, 53906, 107, 50228, 108, 36278, 237, 11372, 243, 11372, 253, 62456, 36278, 249, 11372, 105, 81278, 97, 60008, 36278, 243, 81278, 249, 28025, 223, 36278, 106, 50228, 101, 28025, 223, 11372, 115, 60008, 73358, 36278, 227, 11372, 116, 28025, 234, 11372, 250, 87648, 53906, 107, 11372, 106, 28025, 224, 11372, 110, 11372, 243, 36278, 241, 36278, 243, 28025, 223, 73358, 28025, 223, 11372, 248, 81278, 103, 28025, 224, 73358, 53906, 96, 36278, 106, 87648, 53906, 97, 11372, 105, 53906, 107, 36278, 101, 81278, 107, 11372, 120, 60008, 36278, 105, 53906, 107, 50228, 103, 11372, 243, 36278, 116, 11372, 106, 50228, 110, 28025, 233, 11372, 248, 87648, 42412, 36278, 114, 28025, 223, 73358, 28025, 223, 36278, 117, 11372, 107, 11372, 120, 60008, 11372, 249, 60008, 100278, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_loader = DataLoader(tokenized_train, batch_size=2, collate_fn=data_collator)\nbatch = next(iter(train_loader))\nprint({key: val.shape for key, val in batch.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:14.007334Z","iopub.execute_input":"2025-01-28T15:25:14.007555Z","iopub.status.idle":"2025-01-28T15:25:14.029401Z","shell.execute_reply.started":"2025-01-28T15:25:14.007534Z","shell.execute_reply":"2025-01-28T15:25:14.028742Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': torch.Size([2, 256]), 'attention_mask': torch.Size([2, 256]), 'labels': torch.Size([2, 256])}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"batch = {k: v.to(device) for k, v in batch.items()}\noutputs = model(**batch)\nprint(\"Loss:\", outputs.loss)\nprint(\"Logits shape:\", outputs.logits.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:14.030139Z","iopub.execute_input":"2025-01-28T15:25:14.030405Z","iopub.status.idle":"2025-01-28T15:25:15.209251Z","shell.execute_reply.started":"2025-01-28T15:25:14.030384Z","shell.execute_reply":"2025-01-28T15:25:15.208356Z"}},"outputs":[{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"name":"stdout","text":"Loss: tensor(13.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\nLogits shape: torch.Size([2, 256, 128256])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\n\nbatch = next(iter(train_loader))\nbatch = {k: v.to(device) for k, v in batch.items()}\noutputs = model(**batch)\nprint(\"Logits:\", outputs.logits.shape)\n\n# Compute loss and backpropagate\nloss = outputs.loss\nloss.backward()\nprint(torch.cuda.memory_allocated(device) / 1024**3, \"GB used after backward pass\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:15.210229Z","iopub.execute_input":"2025-01-28T15:25:15.210581Z","iopub.status.idle":"2025-01-28T15:25:15.592903Z","shell.execute_reply.started":"2025-01-28T15:25:15.210547Z","shell.execute_reply":"2025-01-28T15:25:15.592041Z"}},"outputs":[{"name":"stdout","text":"Logits: torch.Size([2, 256, 128256])\n7.015652656555176 GB used after backward pass\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# ------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nprint(\"ran\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:15.593860Z","iopub.execute_input":"2025-01-28T15:25:15.594102Z","iopub.status.idle":"2025-01-28T15:25:15.598454Z","shell.execute_reply.started":"2025-01-28T15:25:15.594079Z","shell.execute_reply":"2025-01-28T15:25:15.597633Z"}},"outputs":[{"name":"stdout","text":"ran\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# for name, param in model.named_parameters():\n#     if not param.requires_grad:\n#         print(f\"Parameter {name} does not require gradients!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:15.599172Z","iopub.execute_input":"2025-01-28T15:25:15.599406Z","iopub.status.idle":"2025-01-28T15:25:15.612997Z","shell.execute_reply.started":"2025-01-28T15:25:15.599385Z","shell.execute_reply":"2025-01-28T15:25:15.612349Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# for param in model.parameters():\n#     param.requires_grad = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:15.613828Z","iopub.execute_input":"2025-01-28T15:25:15.614111Z","iopub.status.idle":"2025-01-28T15:25:15.629175Z","shell.execute_reply.started":"2025-01-28T15:25:15.614079Z","shell.execute_reply":"2025-01-28T15:25:15.628556Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",          # Save model here\n    eval_strategy=\"epoch\",    # Evaluate every epoch\n    learning_rate=1e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    predict_with_generate=False,     # Generate summaries during eval\n    logging_dir='./logs',           # For logging\n    logging_steps=100,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    fp16 = True,\n    dataloader_num_workers=2,        # Optimize data loading\n    gradient_accumulation_steps=4,   # Adjust if batch size is small\n    \n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T15:25:15.629965Z","iopub.execute_input":"2025-01-28T15:25:15.630268Z","iopub.status.idle":"2025-01-28T17:40:30.108801Z","shell.execute_reply.started":"2025-01-28T15:25:15.630237Z","shell.execute_reply":"2025-01-28T17:40:30.107957Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1329' max='1329' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1329/1329 2:15:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.415100</td>\n      <td>3.405940</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.396700</td>\n      <td>3.394353</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.393300</td>\n      <td>3.391777</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1329, training_loss=3.8410122069853245, metrics={'train_runtime': 8113.4271, 'train_samples_per_second': 2.621, 'train_steps_per_second': 0.164, 'total_flos': 4.104423734457139e+16, 'train_loss': 3.8410122069853245, 'epoch': 3.0})"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## Save model?","metadata":{}},{"cell_type":"code","source":"# Save the model and tokenizer\ntrainer.save_model(\"/kaggle/working/saved_model\")  # Save in Kaggle working directory\ntokenizer.save_pretrained(\"/kaggle/working/saved_model\")  # Save tokenizer files\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T17:40:30.109732Z","iopub.execute_input":"2025-01-28T17:40:30.110057Z","iopub.status.idle":"2025-01-28T17:40:31.424941Z","shell.execute_reply.started":"2025-01-28T17:40:30.110032Z","shell.execute_reply":"2025-01-28T17:40:31.423914Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/saved_model/tokenizer_config.json',\n '/kaggle/working/saved_model/special_tokens_map.json',\n '/kaggle/working/saved_model/tokenizer.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Compress the saved model directory\nshutil.make_archive(\"/kaggle/working/saved_model\", 'zip', \"/kaggle/working/saved_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T17:40:31.425938Z","iopub.execute_input":"2025-01-28T17:40:31.426683Z","iopub.status.idle":"2025-01-28T17:40:35.852309Z","shell.execute_reply.started":"2025-01-28T17:40:31.426643Z","shell.execute_reply":"2025-01-28T17:40:35.851605Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/saved_model.zip'"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"# Get ROUGE scores","metadata":{}},{"cell_type":"code","source":"pip install rouge_score evaluate --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T17:40:35.853104Z","iopub.execute_input":"2025-01-28T17:40:35.853446Z","iopub.status.idle":"2025-01-28T17:40:41.324367Z","shell.execute_reply.started":"2025-01-28T17:40:35.853413Z","shell.execute_reply":"2025-01-28T17:40:41.323337Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom nltk.tokenize import RegexpTokenizer\n\nrouge_metric = evaluate.load(\"rouge\")\n\n# define function for custom tokenization\ndef tokenize_sentence(arg):\n    encoded_arg = tokenizer(arg)\n    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n\n# define function to get ROUGE scores with custom tokenization\ndef metrics_func(eval_arg):\n    preds, labels = eval_arg\n\n    # Replace -100\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Convert id tokens to text\n\n    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Insert a line break (\\n) in each sentence for ROUGE scoring\n\n    # (Note : Please change this code, when you perform on other languages except for Bengali)\n    text_preds = [(p if p.endswith((\"!\", \"!\", \"?\", \"?\", \"।\")) else p + \"।\") for p in text_preds]\n    text_labels = [(l if l.endswith((\"!\", \"!\", \"?\", \"?\", \"।\")) else l + \"।\") for l in text_labels]\n    sent_tokenizer_bn = RegexpTokenizer(u'[^!!??।]*[!!??।]')\n    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(p))) for p in text_preds]\n    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(l))) for l in text_labels]\n\n    # compute ROUGE score with custom tokenization\n    return rouge_metric.compute(\n    predictions=text_preds,\n    references=text_labels,\n    tokenizer=tokenize_sentence\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T17:40:41.329088Z","iopub.execute_input":"2025-01-28T17:40:41.329364Z","iopub.status.idle":"2025-01-28T17:40:44.237201Z","shell.execute_reply.started":"2025-01-28T17:40:41.329342Z","shell.execute_reply":"2025-01-28T17:40:44.236357Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1df4c55edc347d8946a6010cc15d7d1"}},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"## Dataloader takes data in random sequence randomly, so results may vary","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntesting_dataloader = DataLoader(\n    tokenized_test.with_format(\"torch\"),\n    collate_fn=data_collator,\n    batch_size=1\n)\n\nfor batch in testing_dataloader:\n    with torch.no_grad():\n        preds = model.generate(\n            batch[\"input_ids\"].to(device),\n            num_beams=2,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            remove_invalid_values=True,\n            max_new_tokens=256\n        )\n    labels = batch[\"labels\"]\n    break\n\nmetrics_func([preds, labels])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T17:40:44.238358Z","iopub.execute_input":"2025-01-28T17:40:44.238672Z","iopub.status.idle":"2025-01-28T17:40:46.784297Z","shell.execute_reply.started":"2025-01-28T17:40:44.238639Z","shell.execute_reply":"2025-01-28T17:40:46.783555Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.6708074534161491,\n 'rouge2': 0.49896049896049893,\n 'rougeL': 0.33126293995859213,\n 'rougeLsum': 0.49897750511247446}"},"metadata":{}}],"execution_count":25}]}