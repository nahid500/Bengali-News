{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:21.613833Z",
     "iopub.status.busy": "2024-10-07T08:23:21.613436Z",
     "iopub.status.idle": "2024-10-07T08:23:21.619468Z",
     "shell.execute_reply": "2024-10-07T08:23:21.618505Z",
     "shell.execute_reply.started": "2024-10-07T08:23:21.613796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer ,DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:21.621882Z",
     "iopub.status.busy": "2024-10-07T08:23:21.621524Z",
     "iopub.status.idle": "2024-10-07T08:23:21.631581Z",
     "shell.execute_reply": "2024-10-07T08:23:21.630598Z",
     "shell.execute_reply.started": "2024-10-07T08:23:21.621847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Restart kernel to clear memory\n",
    "# !kill -9 -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:21.633600Z",
     "iopub.status.busy": "2024-10-07T08:23:21.632802Z",
     "iopub.status.idle": "2024-10-07T08:23:23.108738Z",
     "shell.execute_reply": "2024-10-07T08:23:23.107898Z",
     "shell.execute_reply.started": "2024-10-07T08:23:21.633565Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/bengali-news/xlsum_bengali.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:23.110577Z",
     "iopub.status.busy": "2024-10-07T08:23:23.110136Z",
     "iopub.status.idle": "2024-10-07T08:23:23.127189Z",
     "shell.execute_reply": "2024-10-07T08:23:23.126293Z",
     "shell.execute_reply.started": "2024-10-07T08:23:23.110523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶¶‡ßÅ‡¶∞‡ßç‡¶ó‡¶æ‡¶™‡ßÇ‡¶ú‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá ‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶∂‡ßã‡¶ï ‡¶™‡¶æ‡¶≤‡¶® ‡¶ï‡¶∞‡ßá‡¶® '‡¶Æ‡¶π‡¶ø‡¶∑‡¶æ‡¶∏...</td>\n",
       "      <td>‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡ßÄ‡¶∞‡¶æ ‡¶Ø‡ßá ‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶°‡¶º ‡¶â‡ßé‡¶∏‡¶¨...</td>\n",
       "      <td>‡¶¶‡ßÅ‡¶∞‡ßç‡¶ó‡¶æ‡¶™‡ßÅ‡¶ú‡¶æ‡¶Ø‡¶º ‡¶Æ‡¶π‡¶ø‡¶∑‡¶æ‡¶∏‡ßÅ‡¶∞ ‡¶¨‡¶ß‡ßç‡¶Ø‡ßá ‡¶Æ‡¶ß‡ßç‡¶Ø ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ö‡¶∂‡ßÅ‡¶≠‡¶∞ ‡¶ì...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶∞‡¶æ‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶∞ ‡ß®‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡ßá‡¶ü‡ßá‡¶õ‡ßá ‡¶≠‡ßç‡¶≤‡¶æ‡¶¶‡¶ø‡¶Æ...</td>\n",
       "      <td>‡¶≠‡ßç‡¶≤‡¶æ‡¶¶‡¶ø‡¶Æ‡¶ø‡¶∞ ‡¶™‡ßÅ‡¶§‡¶ø‡¶® ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶Ø‡¶º ‡¶•‡¶æ‡¶ï‡¶æ‡¶∞ ‡ß®‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶™‡ßÇ‡¶∞...</td>\n",
       "      <td>‡¶ó‡¶§ ‡ß®‡ß¶ ‡¶¨‡¶õ‡¶∞‡ßá ‡¶§‡¶ø‡¶®‡¶ø ‡¶∞‡¶æ‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡ßá‡¶∏‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶ß‡¶æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶∏‡¶Ç‡¶∏‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®: ‡¶¨‡¶∞‡¶ø‡¶∂‡¶æ‡¶≤‡ßá ‡¶õ‡¶Ø‡¶º‡¶ü‡¶ø ‡¶Ü‡¶∏‡¶® ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶¶...</td>\n",
       "      <td>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¶‡¶ï‡ßç‡¶∑‡¶ø‡¶£‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßÄ‡¶Ø‡¶º ‡¶ú‡ßá‡¶≤‡¶æ ‡¶¨‡¶∞‡¶ø‡¶∂‡¶æ‡¶≤ ‡¶è‡¶ñ‡¶® ‡¶§‡ßÅ‡¶Æ‡ßÅ...</td>\n",
       "      <td>‡¶¨‡¶∞‡¶ø‡¶∂‡¶æ‡¶≤ ‡¶∏‡¶¶‡¶∞‡ßá ‡¶ö‡¶≤‡¶õ‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞‡¶£‡¶æ‡•§ ‡¶Ø‡¶¶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶∏‡¶∞‡ßç‡¶¨‡¶ï‡¶æ‡¶≤‡ßá‡¶∞ ‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßç‡¶∞‡ßá‡¶∑‡ßç‡¶† ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø: ‡¶¨‡¶ø‡¶¨‡¶ø‡¶∏‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶∞...</td>\n",
       "      <td>‡¶¶‡ßÅ'‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶ø‡¶¨‡¶ø‡¶∏‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø '‡¶∂‡ßç‡¶∞‡ßã‡¶§‡¶æ ‡¶ú‡¶∞...</td>\n",
       "      <td>‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞ ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø‡¶∞ ‡¶ï‡¶æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡ß≠‡¶á ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ö‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶£: ‡ß´‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶∞‡ßá‡¶∏‡¶ï‡ßã‡¶∞‡ßç‡¶∏ ‡¶Æ‡¶Ø‡¶º‡¶¶‡¶æ‡¶®‡ßá ‡¶â...</td>\n",
       "      <td>'‡¶≠‡¶æ‡¶∑‡¶£ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶Ü‡¶ó‡ßá ‡¶Æ‡¶æ‡¶•‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶ø‡¶Æ‡¶æ‡¶® ‡¶Ü‡¶∞ ‡¶π‡ßá‡¶≤‡¶ø‡¶ï‡¶™...</td>\n",
       "      <td>‡¶Ü‡¶∞ ‡¶ï‡ßÅ‡¶Æ‡¶ø‡¶≤‡ßç‡¶≤‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶∏ ‡¶≠‡¶æ‡¶°‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶®‡¶ø‡¶ú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>‡¶ï‡¶∞‡ßã‡¶®‡¶æ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏: ‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶Ü‡¶∂‡¶ô‡ßç‡¶ï‡¶æ, ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶ú‡ßÅ‡¶°‡¶º‡ßá ‡¶Ü‡¶∏‡¶§‡ßá ...</td>\n",
       "      <td>‡¶ï‡¶∞‡ßã‡¶®‡¶æ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏‡ßá ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡ßß‡ß≠‡ß¶ ...</td>\n",
       "      <td>‡¶¨‡¶ø‡¶∂‡ßç‡¶¨ ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ ‡¶¨‡¶≤‡¶õ‡ßá ‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡ßÄ ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá ‡¶™‡ßÅ‡¶∞‡ßã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>‡¶¨‡ßÅ‡¶®‡ßã ‡¶π‡¶æ‡¶§‡¶ø‡¶ü‡¶ø‡¶∞ ‡¶™‡ßá‡¶õ‡¶®‡ßá ‡¶≠‡¶æ‡¶∞‡¶§-‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶®‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ‡¶∞‡¶æ</td>\n",
       "      <td>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ú‡¶æ‡¶Æ‡¶æ‡¶≤‡¶™‡ßÅ‡¶∞‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡¶∞‡ßá ‡¶Ü‡¶ü‡¶ï‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ...</td>\n",
       "      <td>‡¶ú‡¶æ‡¶Æ‡¶æ‡¶≤‡¶™‡ßÅ‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ö‡¶∞‡ßá ‡¶Ü‡¶ü‡¶ï‡ßá ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ‡¶Ø‡¶º ‡¶è...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶õ‡¶æ‡¶°‡¶º‡¶æ ‡¶è‡¶ï‡¶æ ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂...</td>\n",
       "      <td>‡¶è‡¶ñ‡¶® ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨‡ßá‡¶∞ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞‡¶æ ‡¶ï‡ßã‡¶®‡ßã ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï‡ßá...</td>\n",
       "      <td>‡¶≠‡ßç‡¶∞‡¶Æ‡¶£‡ßá‡¶∞ ‡¶Ö‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨‡ßá‡¶∞ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞‡¶ï‡ßá ‡¶™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>‡¶¶‡¶ø‡¶≤‡ßç‡¶≤‡ßÄ‡¶§‡ßá ‡¶ï‡¶ø‡¶∂‡ßã‡¶∞‡¶¶‡ßá‡¶∞ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá ‡¶ó‡¶£‡¶ß‡¶∞‡ßç‡¶∑‡¶£‡ßá‡¶∞...</td>\n",
       "      <td>‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶Ö‡¶™‡ßç‡¶∞‡¶æ‡¶™‡ßç‡¶§‡¶¨‡¶Ø‡¶º‡¶∏‡ßç‡¶ï ‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶...</td>\n",
       "      <td>‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡ßå‡¶® ‡¶π‡¶Ø‡¶º‡¶∞‡¶æ‡¶®‡¶ø‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶ï‡¶∞‡¶æ‡¶Ø‡¶º ‡¶ó...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶á‡¶ó‡¶∞ ‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶¶‡ßá‡¶∞ ‡¶ì‡¶™‡¶∞ ‡¶®‡¶ú‡¶∞‡¶¶‡¶æ‡¶∞‡¶ø ...</td>\n",
       "      <td>‡¶ö‡ßÄ‡¶®‡ßá ‡¶∂‡¶ø‡¶®‡¶ú‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ç ‡¶™‡ßç‡¶∞‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶â‡¶á‡¶ó‡¶∞ ‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ ‡¶ú‡¶®‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ‡¶∞ ...</td>\n",
       "      <td>‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶á‡¶ó‡¶∞‡¶¶‡ßá‡¶∞ ‡¶ì‡¶™‡¶∞ ‡¶®‡¶ú‡¶∞‡¶¶‡¶æ‡¶∞‡¶ø ‡¶ï‡¶∞‡¶õ‡ßá ‡¶ö‡ßÄ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10126 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      ‡¶¶‡ßÅ‡¶∞‡ßç‡¶ó‡¶æ‡¶™‡ßÇ‡¶ú‡¶æ‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá ‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶∂‡ßã‡¶ï ‡¶™‡¶æ‡¶≤‡¶® ‡¶ï‡¶∞‡ßá‡¶® '‡¶Æ‡¶π‡¶ø‡¶∑‡¶æ‡¶∏...   \n",
       "1      ‡¶∞‡¶æ‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶∞ ‡ß®‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡ßá‡¶ü‡ßá‡¶õ‡ßá ‡¶≠‡ßç‡¶≤‡¶æ‡¶¶‡¶ø‡¶Æ...   \n",
       "2      ‡¶∏‡¶Ç‡¶∏‡¶¶ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®: ‡¶¨‡¶∞‡¶ø‡¶∂‡¶æ‡¶≤‡ßá ‡¶õ‡¶Ø‡¶º‡¶ü‡¶ø ‡¶Ü‡¶∏‡¶® ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡¶¨‡¶æ‡¶∞ ‡¶¶...   \n",
       "3      ‡¶∏‡¶∞‡ßç‡¶¨‡¶ï‡¶æ‡¶≤‡ßá‡¶∞ ‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßç‡¶∞‡ßá‡¶∑‡ßç‡¶† ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø: ‡¶¨‡¶ø‡¶¨‡¶ø‡¶∏‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶∞ ‡¶ú‡¶∞...   \n",
       "4      ‡ß≠‡¶á ‡¶Æ‡¶æ‡¶∞‡ßç‡¶ö‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶£: ‡ß´‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶∞‡ßá‡¶∏‡¶ï‡ßã‡¶∞‡ßç‡¶∏ ‡¶Æ‡¶Ø‡¶º‡¶¶‡¶æ‡¶®‡ßá ‡¶â...   \n",
       "...                                                  ...   \n",
       "10121  ‡¶ï‡¶∞‡ßã‡¶®‡¶æ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏: ‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶Ü‡¶∂‡¶ô‡ßç‡¶ï‡¶æ, ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶ú‡ßÅ‡¶°‡¶º‡ßá ‡¶Ü‡¶∏‡¶§‡ßá ...   \n",
       "10122       ‡¶¨‡ßÅ‡¶®‡ßã ‡¶π‡¶æ‡¶§‡¶ø‡¶ü‡¶ø‡¶∞ ‡¶™‡ßá‡¶õ‡¶®‡ßá ‡¶≠‡¶æ‡¶∞‡¶§-‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¨‡¶®‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ‡¶∞‡¶æ   \n",
       "10123  ‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï ‡¶õ‡¶æ‡¶°‡¶º‡¶æ ‡¶è‡¶ï‡¶æ ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂...   \n",
       "10124  ‡¶¶‡¶ø‡¶≤‡ßç‡¶≤‡ßÄ‡¶§‡ßá ‡¶ï‡¶ø‡¶∂‡ßã‡¶∞‡¶¶‡ßá‡¶∞ ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™‡ßá ‡¶ó‡¶£‡¶ß‡¶∞‡ßç‡¶∑‡¶£‡ßá‡¶∞...   \n",
       "10125  ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶á‡¶ó‡¶∞ ‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ‡¶¶‡ßá‡¶∞ ‡¶ì‡¶™‡¶∞ ‡¶®‡¶ú‡¶∞‡¶¶‡¶æ‡¶∞‡¶ø ...   \n",
       "\n",
       "                                                 summary  \\\n",
       "0      ‡¶π‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡ßÄ‡¶∞‡¶æ ‡¶Ø‡ßá ‡¶∏‡¶Æ‡¶Ø‡¶º‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶°‡¶º ‡¶â‡ßé‡¶∏‡¶¨...   \n",
       "1      ‡¶≠‡ßç‡¶≤‡¶æ‡¶¶‡¶ø‡¶Æ‡¶ø‡¶∞ ‡¶™‡ßÅ‡¶§‡¶ø‡¶® ‡¶§‡¶æ‡¶Å‡¶∞ ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶Ø‡¶º ‡¶•‡¶æ‡¶ï‡¶æ‡¶∞ ‡ß®‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶™‡ßÇ‡¶∞...   \n",
       "2      ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶¶‡¶ï‡ßç‡¶∑‡¶ø‡¶£‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßÄ‡¶Ø‡¶º ‡¶ú‡ßá‡¶≤‡¶æ ‡¶¨‡¶∞‡¶ø‡¶∂‡¶æ‡¶≤ ‡¶è‡¶ñ‡¶® ‡¶§‡ßÅ‡¶Æ‡ßÅ...   \n",
       "3      ‡¶¶‡ßÅ'‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶¨‡¶ø‡¶¨‡¶ø‡¶∏‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø '‡¶∂‡ßç‡¶∞‡ßã‡¶§‡¶æ ‡¶ú‡¶∞...   \n",
       "4      '‡¶≠‡¶æ‡¶∑‡¶£ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶Ü‡¶ó‡ßá ‡¶Æ‡¶æ‡¶•‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¨‡¶ø‡¶Æ‡¶æ‡¶® ‡¶Ü‡¶∞ ‡¶π‡ßá‡¶≤‡¶ø‡¶ï‡¶™...   \n",
       "...                                                  ...   \n",
       "10121  ‡¶ï‡¶∞‡ßã‡¶®‡¶æ‡¶≠‡¶æ‡¶á‡¶∞‡¶æ‡¶∏‡ßá ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡ßß‡ß≠‡ß¶ ...   \n",
       "10122  ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ú‡¶æ‡¶Æ‡¶æ‡¶≤‡¶™‡ßÅ‡¶∞‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ö‡¶∞‡ßá ‡¶Ü‡¶ü‡¶ï‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ...   \n",
       "10123  ‡¶è‡¶ñ‡¶® ‡¶•‡ßá‡¶ï‡ßá ‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨‡ßá‡¶∞ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶∞‡¶æ ‡¶ï‡ßã‡¶®‡ßã ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶Ö‡¶≠‡¶ø‡¶≠‡¶æ‡¶¨‡¶ï‡ßá...   \n",
       "10124  ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶æ‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ ‡¶ó‡ßç‡¶∞‡ßÅ‡¶™ ‡¶ö‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶Ö‡¶™‡ßç‡¶∞‡¶æ‡¶™‡ßç‡¶§‡¶¨‡¶Ø‡¶º‡¶∏‡ßç‡¶ï ‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶...   \n",
       "10125  ‡¶ö‡ßÄ‡¶®‡ßá ‡¶∂‡¶ø‡¶®‡¶ú‡¶ø‡¶Ø‡¶º‡¶æ‡¶Ç ‡¶™‡ßç‡¶∞‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶â‡¶á‡¶ó‡¶∞ ‡¶Æ‡ßÅ‡¶∏‡¶≤‡¶ø‡¶Æ ‡¶ú‡¶®‡¶ó‡ßã‡¶∑‡ßç‡¶†‡ßÄ‡¶∞ ...   \n",
       "\n",
       "                                                 article  \n",
       "0      ‡¶¶‡ßÅ‡¶∞‡ßç‡¶ó‡¶æ‡¶™‡ßÅ‡¶ú‡¶æ‡¶Ø‡¶º ‡¶Æ‡¶π‡¶ø‡¶∑‡¶æ‡¶∏‡ßÅ‡¶∞ ‡¶¨‡¶ß‡ßç‡¶Ø‡ßá ‡¶Æ‡¶ß‡ßç‡¶Ø ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ö‡¶∂‡ßÅ‡¶≠‡¶∞ ‡¶ì...  \n",
       "1      ‡¶ó‡¶§ ‡ß®‡ß¶ ‡¶¨‡¶õ‡¶∞‡ßá ‡¶§‡¶ø‡¶®‡¶ø ‡¶∞‡¶æ‡¶∂‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡ßá‡¶∏‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶™‡ßç‡¶∞‡¶ß‡¶æ...  \n",
       "2      ‡¶¨‡¶∞‡¶ø‡¶∂‡¶æ‡¶≤ ‡¶∏‡¶¶‡¶∞‡ßá ‡¶ö‡¶≤‡¶õ‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßÄ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ö‡¶æ‡¶∞‡¶£‡¶æ‡•§ ‡¶Ø‡¶¶...  \n",
       "3      ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞ ‡¶∞‡¶¨‡ßÄ‡¶®‡ßç‡¶¶‡ßç‡¶∞‡¶®‡¶æ‡¶• ‡¶†‡¶æ‡¶ï‡ßÅ‡¶∞ ‡¶¨‡¶æ‡¶ô‡¶æ‡¶≤‡¶ø‡¶∞ ‡¶ï‡¶æ...  \n",
       "4      ‡¶Ü‡¶∞ ‡¶ï‡ßÅ‡¶Æ‡¶ø‡¶≤‡ßç‡¶≤‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶∏ ‡¶≠‡¶æ‡¶°‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶Ö‡¶®‡ßá‡¶ï‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶®‡¶ø‡¶ú...  \n",
       "...                                                  ...  \n",
       "10121  ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨ ‡¶∏‡ßç‡¶¨‡¶æ‡¶∏‡ßç‡¶•‡ßç‡¶Ø ‡¶∏‡¶Ç‡¶∏‡ßç‡¶•‡¶æ ‡¶¨‡¶≤‡¶õ‡ßá ‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡ßÄ ‡¶Ü‡¶ï‡¶æ‡¶∞‡ßá ‡¶™‡ßÅ‡¶∞‡ßã...  \n",
       "10122  ‡¶ú‡¶æ‡¶Æ‡¶æ‡¶≤‡¶™‡ßÅ‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ö‡¶∞‡ßá ‡¶Ü‡¶ü‡¶ï‡ßá ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶≠‡¶æ‡¶∞‡¶§‡ßÄ‡¶Ø‡¶º ‡¶è...  \n",
       "10123  ‡¶≠‡ßç‡¶∞‡¶Æ‡¶£‡ßá‡¶∞ ‡¶Ö‡¶ß‡¶ø‡¶ï‡¶æ‡¶∞‡ßá‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶∏‡ßå‡¶¶‡¶ø ‡¶Ü‡¶∞‡¶¨‡ßá‡¶∞ ‡¶®‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞‡¶ï‡ßá ‡¶™...  \n",
       "10124  ‡¶Æ‡ßá‡¶Ø‡¶º‡ßá‡¶¶‡ßá‡¶∞ ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ø‡ßå‡¶® ‡¶π‡¶Ø‡¶º‡¶∞‡¶æ‡¶®‡¶ø‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶Æ‡¶®‡ßç‡¶§‡¶¨‡ßç‡¶Ø ‡¶ï‡¶∞‡¶æ‡¶Ø‡¶º ‡¶ó...  \n",
       "10125  ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶á‡¶ó‡¶∞‡¶¶‡ßá‡¶∞ ‡¶ì‡¶™‡¶∞ ‡¶®‡¶ú‡¶∞‡¶¶‡¶æ‡¶∞‡¶ø ‡¶ï‡¶∞‡¶õ‡ßá ‡¶ö‡ßÄ...  \n",
       "\n",
       "[10126 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:23.129889Z",
     "iopub.status.busy": "2024-10-07T08:23:23.129592Z",
     "iopub.status.idle": "2024-10-07T08:23:23.143805Z",
     "shell.execute_reply": "2024-10-07T08:23:23.143053Z",
     "shell.execute_reply.started": "2024-10-07T08:23:23.129858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['article', 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:23.145189Z",
     "iopub.status.busy": "2024-10-07T08:23:23.144840Z",
     "iopub.status.idle": "2024-10-07T08:23:23.168397Z",
     "shell.execute_reply": "2024-10-07T08:23:23.167546Z",
     "shell.execute_reply.started": "2024-10-07T08:23:23.145144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.30, shuffle=True)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.65,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:23.170282Z",
     "iopub.status.busy": "2024-10-07T08:23:23.169931Z",
     "iopub.status.idle": "2024-10-07T08:23:23.174062Z",
     "shell.execute_reply": "2024-10-07T08:23:23.173146Z",
     "shell.execute_reply.started": "2024-10-07T08:23:23.170242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# ds_train = Dataset.from_pandas(df_train)\n",
    "# ds_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:23.175603Z",
     "iopub.status.busy": "2024-10-07T08:23:23.175305Z",
     "iopub.status.idle": "2024-10-07T08:23:23.186958Z",
     "shell.execute_reply": "2024-10-07T08:23:23.186092Z",
     "shell.execute_reply.started": "2024-10-07T08:23:23.175566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡¶§‡¶ø‡¶∞‡¶ø‡¶∂ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡ßß‡ßØ‡ßØ‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá‡¶∞ ‡ß¨‡¶á ‡¶°‡¶ø‡¶∏‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶è‡¶ï ‡¶§‡ßÄ‡¶¨‡ßç‡¶∞ ‡¶ó‡¶£‡¶Ü‡¶®‡ßç‡¶¶‡ßã‡¶≤‡¶®‡ßá‡¶∞ ‡¶Æ‡ßÅ‡¶ñ‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶≤ ‡¶è‡¶∞‡¶∂‡¶æ‡¶¶‡ßá‡¶∞ ‡¶®‡¶Ø‡¶º ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶∂‡¶æ‡¶∏‡¶®‡ßá‡¶∞ ‡¶Ö‡¶¨‡¶∏‡¶æ‡¶® ‡¶ò‡¶ü‡ßá‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶§‡¶æ‡¶∞‡¶ì ‡¶§‡¶ø‡¶® ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶Ü‡¶∞‡ßá‡¶ï‡¶ü‡¶ø ‡¶ó‡¶£‡¶Ü‡¶®‡ßç‡¶¶‡ßã‡¶≤‡¶® ‡¶§‡¶ø‡¶®‡¶ø ‡¶®‡¶ø‡¶∑‡ßç‡¶†‡ßÅ‡¶∞‡¶≠‡¶æ‡¶¨‡ßá ‡¶¶‡¶Æ‡¶® ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡¶≤‡ßá‡¶®‡•§ ‡¶∏‡ßá‡¶á ‡¶Ü‡¶®‡ßç‡¶¶‡ßã‡¶≤‡¶®‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ó‡¶£‡¶§‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßá‡¶∞ ‡¶¶‡¶æ‡¶¨‡¶ø‡¶§‡ßá ‡¶¨‡ßÅ‡¶ï‡ßá-‡¶™‡¶ø‡¶†‡ßá ‡¶∂‡ßç‡¶≤‡ßã‡¶ó‡¶æ‡¶® ‡¶≤‡¶ø‡¶ñ‡ßá ‡¶∞‡¶æ‡¶∏‡ßç‡¶§‡¶æ‡¶Ø‡¶º ‡¶®‡¶æ‡¶Æ‡¶æ ‡¶è‡¶ï ‡¶§‡¶∞‡ßÅ‡¶£ ‡¶®‡ßÇ‡¶∞ ‡¶π‡ßã‡¶∏‡ßá‡¶® ‡¶™‡ßÅ‡¶≤‡¶ø‡¶∂‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶§‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡¶π‡¶®‡•§ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶§‡ßã‡¶≤‡¶æ ‡¶§‡¶æ‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶õ‡¶¨‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶ó‡¶£‡¶§‡¶æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶ø‡¶ï ‡¶Ü‡¶®‡ßç‡¶¶‡ßã‡¶≤‡¶®‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡ßÄ‡¶ï ‡¶π‡¶Ø‡¶º‡ßá ‡¶â‡¶†‡ßá ‡¶è‡¶¨‡¶Ç ‡¶§‡¶ø‡¶® ‡¶¨‡¶õ‡¶∞ ‡¶™‡¶∞‡ßá‡¶∞ ‡¶ó‡¶£‡¶Ü‡¶®‡ßç‡¶¶‡ßã‡¶≤‡¶®‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡¶ø‡¶§ ‡¶ï‡¶∞‡ßá‡•§ ‡ßß‡ßØ‡ßÆ‡ß≠ ‡¶∏‡¶æ‡¶≤‡ßá‡¶∞ ‡ßß‡ß¶ ‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶®‡ßÇ‡¶∞ ‡¶π‡ßã‡¶∏‡ßá‡¶®‡ßá‡¶∞ ‡¶¨‡ßÅ‡¶≤‡ßá‡¶ü‡¶¨‡¶ø‡¶¶‡ßç‡¶ß ‡¶¶‡ßá‡¶π ‡¶Ø‡ßá ‡¶ï‡¶æ‡¶∞‡¶æ‡¶ï‡¶ï‡ßç‡¶∑‡ßá ‡¶´‡ßá‡¶≤‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶Ø‡¶º, ‡¶§‡¶æ‡¶∞ ‡¶™‡¶æ‡¶∂‡ßá‡¶∞ ‡¶ï‡¶ï‡ßç‡¶∑‡ßá ‡¶¨‡¶®‡ßç‡¶¶‡ßÄ ‡¶õ‡¶ø‡¶≤‡ßá‡¶® ‡¶¨‡¶ø‡¶¨‡¶ø‡¶∏‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶∞ ‡¶Æ‡ßã‡¶Ø‡¶º‡¶æ‡¶ú‡ßç‡¶ú‡ßá‡¶Æ ‡¶π‡ßã‡¶∏‡ßá‡¶®‡•§ ‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂ ‡¶¨‡¶õ‡¶∞ ‡¶™‡¶∞ ‡¶®‡ßÇ‡¶∞ ‡¶π‡ßã‡¶∏‡ßá‡¶®‡ßá‡¶∞ ‡¶∏‡ßá‡¶á ‡¶Ö‡¶≠‡¶ø‡¶®‡¶¨ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶¶‡ßá‡¶∞ ‡¶®‡ßá‡¶™‡¶•‡ßç‡¶Ø‡ßá ‡¶ï‡¶æ‡¶π‡¶ø‡¶®‡ßÄ ‡¶ú‡¶æ‡¶®‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡¶§‡¶ø‡¶®‡¶ø‡•§'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['summary'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:23.188909Z",
     "iopub.status.busy": "2024-10-07T08:23:23.188107Z",
     "iopub.status.idle": "2024-10-07T08:23:40.676626Z",
     "shell.execute_reply": "2024-10-07T08:23:40.675576Z",
     "shell.execute_reply.started": "2024-10-07T08:23:23.188864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fd8cb07e864160b57d902689bc75bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0866226a1334dfeadb364acb7c46572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e17fba96bb048199ad053789fad60ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235d62e43381435eb69e3dd4445785f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a6f7da9e0540048735c06d5ea510af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58888bd6a5c4343b12907a2538f443f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "import os\n",
    "\n",
    "\n",
    "# Specify the model name for mT5\n",
    "model_name = \"google/mt5-base\" \n",
    "# Load the mT5 model\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load the mT5 tokenizer\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:40.679553Z",
     "iopub.status.busy": "2024-10-07T08:23:40.678111Z",
     "iopub.status.idle": "2024-10-07T08:23:40.685588Z",
     "shell.execute_reply": "2024-10-07T08:23:40.684680Z",
     "shell.execute_reply.started": "2024-10-07T08:23:40.679504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/mt5-base'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:40.687648Z",
     "iopub.status.busy": "2024-10-07T08:23:40.686974Z",
     "iopub.status.idle": "2024-10-07T08:23:43.994086Z",
     "shell.execute_reply": "2024-10-07T08:23:43.992945Z",
     "shell.execute_reply.started": "2024-10-07T08:23:40.687602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        # Drop rows with NaN values in article or summary columns\n",
    "        data = data.dropna(subset=['article', 'summary'])\n",
    "\n",
    "        # Tokenize input text and labels (without normalization)\n",
    "        self.input_text = data['article'].tolist()\n",
    "        self.labels = data['summary'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the input and labels\n",
    "        inputs = self.tokenizer(self.input_text[idx], \n",
    "                                max_length=self.max_length, \n",
    "                                padding='max_length', \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "\n",
    "        labels = self.tokenizer(self.labels[idx], \n",
    "                                max_length=self.max_length, \n",
    "                                padding='max_length', \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "\n",
    "        # Return input and labels for the sample\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(), \n",
    "            'attention_mask': inputs['attention_mask'].squeeze(), \n",
    "            'labels': labels['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:43.996345Z",
     "iopub.status.busy": "2024-10-07T08:23:43.995532Z",
     "iopub.status.idle": "2024-10-07T08:23:44.105208Z",
     "shell.execute_reply": "2024-10-07T08:23:44.103838Z",
     "shell.execute_reply.started": "2024-10-07T08:23:43.996299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MyDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        batch = {}\n",
    "\n",
    "        # Handle input_ids and attention_mask\n",
    "        batch[\"input_ids\"] = torch.stack([feature[\"input_ids\"] for feature in features])\n",
    "        batch[\"attention_mask\"] = torch.stack([feature[\"attention_mask\"] for feature in features])\n",
    "\n",
    "        # Check if the labels are PyTorch tensors\n",
    "        if isinstance(features[0][\"labels\"], torch.Tensor):\n",
    "            # Ensure padding for labels is handled correctly\n",
    "            batch[\"labels\"] = torch.stack([feature[\"labels\"] for feature in features])\n",
    "        else:\n",
    "            # Convert list of lists (if not tensor) to tensor and pad\n",
    "            batch[\"labels\"] = torch.tensor([feature[\"labels\"] for feature in features], dtype=torch.long)\n",
    "\n",
    "        # Make sure labels are padded if necessary (for variable-length sequences)\n",
    "        if self.label_pad_token_id is not None:\n",
    "            batch[\"labels\"] = pad_sequence([feature[\"labels\"] for feature in features], \n",
    "                                           batch_first=True, \n",
    "                                           padding_value=self.label_pad_token_id)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:44.107392Z",
     "iopub.status.busy": "2024-10-07T08:23:44.106950Z",
     "iopub.status.idle": "2024-10-07T08:23:44.235093Z",
     "shell.execute_reply": "2024-10-07T08:23:44.234090Z",
     "shell.execute_reply.started": "2024-10-07T08:23:44.107329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create train, test, and validation datasets\n",
    "train_dataset = Seq2SeqDataset(df_train, tokenizer)\n",
    "test_dataset = Seq2SeqDataset(df_test, tokenizer)\n",
    "validation_dataset = Seq2SeqDataset(df_val, tokenizer)\n",
    "\n",
    "# Create DataLoader for each dataset\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=16, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=MyDataCollatorForSeq2Seq(tokenizer))\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=16, \n",
    "                             collate_fn=MyDataCollatorForSeq2Seq(tokenizer))\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset, \n",
    "                                   batch_size=16, \n",
    "                                   collate_fn=MyDataCollatorForSeq2Seq(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:44.239702Z",
     "iopub.status.busy": "2024-10-07T08:23:44.239344Z",
     "iopub.status.idle": "2024-10-07T08:23:44.366437Z",
     "shell.execute_reply": "2024-10-07T08:23:44.365396Z",
     "shell.execute_reply.started": "2024-10-07T08:23:44.239670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Move the model to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:44.368583Z",
     "iopub.status.busy": "2024-10-07T08:23:44.368214Z",
     "iopub.status.idle": "2024-10-07T08:23:44.392068Z",
     "shell.execute_reply": "2024-10-07T08:23:44.391168Z",
     "shell.execute_reply.started": "2024-10-07T08:23:44.368536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:44.393842Z",
     "iopub.status.busy": "2024-10-07T08:23:44.393471Z",
     "iopub.status.idle": "2024-10-07T08:23:44.827518Z",
     "shell.execute_reply": "2024-10-07T08:23:44.826522Z",
     "shell.execute_reply.started": "2024-10-07T08:23:44.393801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a custom optimizer using torch.optim.AdamW\n",
    "custom_optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:24:17.972409Z",
     "iopub.status.busy": "2024-10-07T08:24:17.971970Z",
     "iopub.status.idle": "2024-10-07T08:24:18.003111Z",
     "shell.execute_reply": "2024-10-07T08:24:18.002001Z",
     "shell.execute_reply.started": "2024-10-07T08:24:17.972356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the TrainingArguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/',\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    save_steps=5000,\n",
    "    learning_rate=1e-3,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=False,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/kaggle/working/',\n",
    "    logging_steps=200,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:23:44.883803Z",
     "iopub.status.busy": "2024-10-07T08:23:44.883503Z",
     "iopub.status.idle": "2024-10-07T08:23:44.888040Z",
     "shell.execute_reply": "2024-10-07T08:23:44.887061Z",
     "shell.execute_reply.started": "2024-10-07T08:23:44.883771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a7eadb1b11af8e041a6e1104b836b061cb0ff681\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:24:22.489081Z",
     "iopub.status.busy": "2024-10-07T08:24:22.488543Z",
     "iopub.status.idle": "2024-10-07T08:24:22.495437Z",
     "shell.execute_reply": "2024-10-07T08:24:22.493977Z",
     "shell.execute_reply.started": "2024-10-07T08:24:22.489028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = MyDataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,  # Padding to the model's max length\n",
    "    max_length=128,  # Use the same max length as during tokenization\n",
    "    label_pad_token_id=tokenizer.pad_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:24:34.470321Z",
     "iopub.status.busy": "2024-10-07T08:24:34.469319Z",
     "iopub.status.idle": "2024-10-07T08:24:34.494694Z",
     "shell.execute_reply": "2024-10-07T08:24:34.493730Z",
     "shell.execute_reply.started": "2024-10-07T08:24:34.470276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers.trainer import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def _save(self, output_dir=None, state_dict=None):\n",
    "        # Ensure all tensors are contiguous before saving\n",
    "        for param in self.model.parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.data.contiguous()\n",
    "        \n",
    "        # Continue with normal save operation\n",
    "        super()._save(output_dir, state_dict)\n",
    "\n",
    "# Use CustomTrainer instead of Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    optimizers=(custom_optimizer, None),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T08:24:37.515760Z",
     "iopub.status.busy": "2024-10-07T08:24:37.515334Z",
     "iopub.status.idle": "2024-10-07T09:20:57.544776Z",
     "shell.execute_reply": "2024-10-07T09:20:57.543802Z",
     "shell.execute_reply.started": "2024-10-07T08:24:37.515719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1326' max='1326' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1326/1326 56:17, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7.364100</td>\n",
       "      <td>2.823433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.558500</td>\n",
       "      <td>1.385688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.310800</td>\n",
       "      <td>1.305233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.263300</td>\n",
       "      <td>1.304631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1326, training_loss=2.4029517655638726, metrics={'train_runtime': 3379.6197, 'train_samples_per_second': 12.584, 'train_steps_per_second': 0.392, 'total_flos': 1.2719481365200896e+16, 'train_loss': 2.4029517655638726, 'epoch': 5.986455981941309})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:21:10.885792Z",
     "iopub.status.busy": "2024-10-07T09:21:10.885369Z",
     "iopub.status.idle": "2024-10-07T09:21:25.743011Z",
     "shell.execute_reply": "2024-10-07T09:21:25.741769Z",
     "shell.execute_reply.started": "2024-10-07T09:21:10.885755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f452fc5be6ac52144971ee24d5c3e4d1306a1aa561f746f90917aca0ec8063c2\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:21:25.746466Z",
     "iopub.status.busy": "2024-10-07T09:21:25.745622Z",
     "iopub.status.idle": "2024-10-07T09:21:38.110873Z",
     "shell.execute_reply": "2024-10-07T09:21:38.109452Z",
     "shell.execute_reply.started": "2024-10-07T09:21:25.746413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:21:38.113028Z",
     "iopub.status.busy": "2024-10-07T09:21:38.112589Z",
     "iopub.status.idle": "2024-10-07T09:21:50.141877Z",
     "shell.execute_reply": "2024-10-07T09:21:50.140573Z",
     "shell.execute_reply.started": "2024-10-07T09:21:38.112976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:21:50.145507Z",
     "iopub.status.busy": "2024-10-07T09:21:50.145121Z",
     "iopub.status.idle": "2024-10-07T09:24:32.312289Z",
     "shell.execute_reply": "2024-10-07T09:24:32.311293Z",
     "shell.execute_reply.started": "2024-10-07T09:21:50.145468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Generated Summaries Count: 1975\n",
      "Filtered References Count: 1975\n",
      "BLEU Score: 0.04643894758914907\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Smoothing function for BLEU\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "# Define the move_to_device function\n",
    "def move_to_device(batch, device):\n",
    "    if isinstance(batch, torch.Tensor):\n",
    "        return batch.to(device)\n",
    "    elif isinstance(batch, list):\n",
    "        return [move_to_device(item, device) for item in batch]\n",
    "    elif isinstance(batch, dict):\n",
    "        return {key: move_to_device(value, device) for key, value in batch.items()}\n",
    "    else:\n",
    "        return batch\n",
    "\n",
    "# Initialize lists to store generated summaries and references\n",
    "generated_summaries = []\n",
    "references = []\n",
    "\n",
    "# Generate summaries for the test dataset\n",
    "for batch in test_dataloader:\n",
    "    # Move the batch to CUDA\n",
    "    batch = move_to_device(batch, 'cuda')\n",
    "\n",
    "    input_text = batch['input_ids']\n",
    "    labels = batch['labels']\n",
    "\n",
    "    # Generate summaries\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            input_text,\n",
    "            num_beams=6,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    # Move the summary_ids to CPU to decode\n",
    "    summary_ids = summary_ids.to('cpu')\n",
    "\n",
    "    # Decode generated summaries and labels\n",
    "    generated_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "    reference_summary = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Extend lists with generated summaries and references\n",
    "    generated_summaries.extend(generated_summary)\n",
    "    references.extend(reference_summary)\n",
    "\n",
    "# Filter out empty predictions and references\n",
    "filtered_generated_summaries = []\n",
    "filtered_references = []\n",
    "\n",
    "for pred, ref in zip(generated_summaries, references):\n",
    "    if pred.strip() and ref.strip():  # Check if both are non-empty\n",
    "        filtered_generated_summaries.append(pred)\n",
    "        filtered_references.append(ref)\n",
    "\n",
    "# Ensure the filtered lists are populated before calculating BLEU and ROUGE\n",
    "print(f\"Filtered Generated Summaries Count: {len(filtered_generated_summaries)}\")\n",
    "print(f\"Filtered References Count: {len(filtered_references)}\")\n",
    "\n",
    "# BLEU score calculation\n",
    "def calculate_bleu(filtered_generated_summaries, filtered_references, tokenizer):\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(filtered_generated_summaries, filtered_references):\n",
    "        pred_tokens = tokenizer.tokenize(pred)\n",
    "        ref_tokens = tokenizer.tokenize(ref)\n",
    "        if pred_tokens and ref_tokens:  # Ensure both are non-empty\n",
    "            score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(score)\n",
    "\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    return avg_bleu_score\n",
    "\n",
    "# Calculate BLEU\n",
    "bleu_score = calculate_bleu(filtered_generated_summaries, filtered_references, tokenizer)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:25:22.257299Z",
     "iopub.status.busy": "2024-10-07T09:25:22.256565Z",
     "iopub.status.idle": "2024-10-07T09:25:25.041427Z",
     "shell.execute_reply": "2024-10-07T09:25:25.040431Z",
     "shell.execute_reply.started": "2024-10-07T09:25:22.257255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Initialize the Rouge scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "# Define a function to preprocess and tokenize Bengali text\n",
    "def preprocess_text(text):\n",
    "    text = unidecode(text)\n",
    "    tokens = text.split()\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Create lists to store individual scores\n",
    "rouge1_f1_scores = []\n",
    "rouge1_precision_scores = []\n",
    "rouge1_recall_scores = []\n",
    "rouge2_f1_scores = []\n",
    "rouge2_precision_scores = []\n",
    "rouge2_recall_scores = []\n",
    "rougeL_f1_scores = []\n",
    "rougeL_precision_scores = []\n",
    "rougeL_recall_scores = []\n",
    "\n",
    "for ref, pred in zip(references, generated_summaries):\n",
    "    candidate = preprocess_text(pred)\n",
    "    reference = preprocess_text(' '.join(ref))\n",
    "    scores = scorer.score(reference, candidate)\n",
    "\n",
    "    rouge1_f1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge1_precision_scores.append(scores['rouge1'].precision)\n",
    "    rouge1_recall_scores.append(scores['rouge1'].recall)\n",
    "    rouge2_f1_scores.append(scores['rouge2'].fmeasure)\n",
    "    rouge2_precision_scores.append(scores['rouge2'].precision)\n",
    "    rouge2_recall_scores.append(scores['rouge2'].recall)\n",
    "    rougeL_f1_scores.append(scores['rougeL'].fmeasure)\n",
    "    rougeL_precision_scores.append(scores['rougeL'].precision)\n",
    "    rougeL_recall_scores.append(scores['rougeL'].recall)\n",
    "\n",
    "# Calculate the average scores\n",
    "avg_rouge1_f1 = sum(rouge1_f1_scores) / len(rouge1_f1_scores)\n",
    "avg_rouge1_precision = sum(rouge1_precision_scores) / len(rouge1_precision_scores)\n",
    "avg_rouge1_recall = sum(rouge1_recall_scores) / len(rouge1_recall_scores)\n",
    "avg_rouge2_f1 = sum(rouge2_f1_scores) / len(rouge2_f1_scores)\n",
    "avg_rouge2_precision = sum(rouge2_precision_scores) / len(rouge2_precision_scores)\n",
    "avg_rouge2_recall = sum(rouge2_recall_scores) / len(rouge2_recall_scores)\n",
    "avg_rougeL_f1 = sum(rougeL_f1_scores) / len(rougeL_f1_scores)\n",
    "avg_rougeL_precision = sum(rougeL_precision_scores) / len(rougeL_precision_scores)\n",
    "avg_rougeL_recall = sum(rougeL_recall_scores) / len(rougeL_recall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T09:25:33.157700Z",
     "iopub.status.busy": "2024-10-07T09:25:33.157281Z",
     "iopub.status.idle": "2024-10-07T09:25:33.164262Z",
     "shell.execute_reply": "2024-10-07T09:25:33.163274Z",
     "shell.execute_reply.started": "2024-10-07T09:25:33.157662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rouge-1 F1 Score: 0.0177801176599677\n",
      "Average Rouge-1 Precision: 0.10865618207390616\n",
      "Average Rouge-1 Recall: 0.009833039682601427\n",
      "Average Rouge-2 F1 Score: 0.0\n",
      "Average Rouge-2 Precision: 0.0\n",
      "Average Rouge-2 Recall: 0.0\n",
      "Average Rouge-L F1 Score: 0.0177801176599677\n",
      "Average Rouge-L Precision: 0.10865618207390616\n",
      "Average Rouge-L Recall: 0.009833039682601427\n"
     ]
    }
   ],
   "source": [
    "# Print the average scores\n",
    "print(\"Average Rouge-1 F1 Score:\", avg_rouge1_f1)\n",
    "print(\"Average Rouge-1 Precision:\", avg_rouge1_precision)\n",
    "print(\"Average Rouge-1 Recall:\", avg_rouge1_recall)\n",
    "\n",
    "print(\"Average Rouge-2 F1 Score:\", avg_rouge2_f1)\n",
    "print(\"Average Rouge-2 Precision:\", avg_rouge2_precision)\n",
    "print(\"Average Rouge-2 Recall:\", avg_rouge2_recall)\n",
    "\n",
    "print(\"Average Rouge-L F1 Score:\", avg_rougeL_f1)\n",
    "print(\"Average Rouge-L Precision:\", avg_rougeL_precision)\n",
    "print(\"Average Rouge-L Recall:\", avg_rougeL_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-07T09:24:32.422820Z",
     "iopub.status.idle": "2024-10-07T09:24:32.423198Z",
     "shell.execute_reply": "2024-10-07T09:24:32.423033Z",
     "shell.execute_reply.started": "2024-10-07T09:24:32.423014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generated_summaries"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5727178,
     "sourceId": 9427733,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
