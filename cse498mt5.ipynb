{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9427733,"sourceType":"datasetVersion","datasetId":5727178}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom transformers import MT5ForConditionalGeneration, AutoTokenizer ,DataCollatorForSeq2Seq, Trainer, TrainingArguments\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:33:45.888744Z","iopub.execute_input":"2024-10-02T11:33:45.889114Z","iopub.status.idle":"2024-10-02T11:34:05.579569Z","shell.execute_reply.started":"2024-10-02T11:33:45.889054Z","shell.execute_reply":"2024-10-02T11:34:05.578754Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# # Restart kernel to clear memory\n# !kill -9 -1\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:05.581462Z","iopub.execute_input":"2024-10-02T11:34:05.582031Z","iopub.status.idle":"2024-10-02T11:34:05.585765Z","shell.execute_reply.started":"2024-10-02T11:34:05.581997Z","shell.execute_reply":"2024-10-02T11:34:05.584869Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cleaned/preprocessed_bangla_news.csv', usecols=['clean_summary', 'clean_article'])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:05.586880Z","iopub.execute_input":"2024-10-02T11:34:05.587655Z","iopub.status.idle":"2024-10-02T11:34:12.901665Z","shell.execute_reply.started":"2024-10-02T11:34:05.587623Z","shell.execute_reply":"2024-10-02T11:34:12.900852Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:12.902771Z","iopub.execute_input":"2024-10-02T11:34:12.903061Z","iopub.status.idle":"2024-10-02T11:34:12.934925Z","shell.execute_reply.started":"2024-10-02T11:34:12.903030Z","shell.execute_reply":"2024-10-02T11:34:12.934048Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                           clean_article  \\\n0      ‡¶§‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßÄ‡ßü ‡¶á‡¶ú‡¶Æ‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡ßÅ‡¶á‡¶°‡ßá‡¶®‡ßá‡¶∞ ...   \n1      ‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶°‡ßá‡¶ô‡ßç‡¶ó‡ßÅ ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø ‡¶¶‡¶ø‡¶® ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶∞‡¶ì ‡¶≠‡ßü‡¶æ‡¶¨‡¶π ‡¶∞ ...   \n2      ‡¶∂‡ßã‡¶ï‡¶æ‡¶¨‡¶π ‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶¶‡¶ø‡¶®‡ßá ‡¶∏‡ßÅ‡¶®‡¶æ‡¶Æ‡¶ó‡¶û‡ßç‡¶ú‡ßá ‡¶∏‡ßç‡¶¨‡ßá‡¶ö‡ßç‡¶õ‡¶æ‡ßü...   \n3      ‡¶™‡¶æ‡¶®‡¶ø‡¶∏‡¶Æ‡ßç‡¶™‡¶¶ ‡¶â‡¶™‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶ï‡ßá‡¶è‡¶Æ ‡¶è‡¶®‡¶æ‡¶Æ‡ßÅ‡¶≤ ‡¶π‡¶ï ‡¶∂‡¶æ‡¶Æ‡ßÄ‡¶Æ ‡¶¨‡¶≤‡ßá‡¶õ...   \n4      ‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø‡¶∞ ‡¶Ö‡¶ó‡ßç‡¶®‡¶ø‡¶∏‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶æ‡¶∏ ‡¶ì ‡¶®‡ßà‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø ‡¶∏‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨...   \n...                                                  ...   \n35111  ‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡ßá‡¶∏‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡ßá‡¶∂‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡ßÄ ...   \n35112  ‡¶∞‡¶Æ‡¶ú‡¶æ‡¶® ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶∏‡¶ø‡¶≤‡ßá‡¶ü ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßÅ‡ßú‡¶ø‡¶∞ ‡¶Æ‡ßá‡ßü‡ßá ‡¶¨‡¶æ‡ßú‡¶ø ‡¶á‡¶´‡¶§‡¶æ‡¶∞‡¶ø...   \n35113  ‡¶¨‡¶ô‡ßç‡¶ó‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶∂‡ßá‡¶ñ ‡¶Æ‡ßÅ‡¶ú‡¶ø‡¶¨‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶æ‡¶Æ‡¶Ø‡ßá ‡¶®‡¶æ‡¶Æ...   \n35114  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶á ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∑‡ßç‡¶ü ‡¶®‡¶ø‡ßü‡ßá ‡¶ö‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá ‡¶∏‡ßá ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡¶§ ‡¶§‡¶æ‡¶ï...   \n35115  ‡¶∏‡ßá‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶è‡¶á ‡¶∂‡ßç‡¶≤‡ßã‡¶ó‡¶æ‡¶®‡ßá ‡¶∂‡¶§‡¶≠‡¶æ‡¶ó ‡¶Æ‡ßá‡¶ß‡¶æ‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶§...   \n\n                                           clean_summary  \n0      ‡¶§‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßÄ‡ßü ‡¶á‡¶ú‡¶Æ‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡ßÅ‡¶á‡¶°‡ßá‡¶®‡ßá‡¶∞ ...  \n1      ‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶°‡ßá‡¶ô‡ßç‡¶ó‡ßÅ ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø ‡¶¶‡¶ø‡¶® ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶∞‡¶ì ‡¶≠‡ßü‡¶æ‡¶¨‡¶π ‡¶∞ ...  \n2      ‡¶∂‡ßã‡¶ï‡¶æ‡¶¨‡¶π ‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶¶‡¶ø‡¶®‡ßá ‡¶∏‡ßÅ‡¶®‡¶æ‡¶Æ‡¶ó‡¶û‡ßç‡¶ú‡ßá ‡¶∏‡ßç‡¶¨‡ßá‡¶ö‡ßç‡¶õ‡¶æ‡ßü...  \n3      ‡¶™‡¶æ‡¶®‡¶ø‡¶∏‡¶Æ‡ßç‡¶™‡¶¶ ‡¶â‡¶™‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶ï‡ßá‡¶è‡¶Æ ‡¶è‡¶®‡¶æ‡¶Æ‡ßÅ‡¶≤ ‡¶π‡¶ï ‡¶∂‡¶æ‡¶Æ‡ßÄ‡¶Æ ‡¶¨‡¶≤‡ßá‡¶õ...  \n4      ‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø‡¶∞ ‡¶Ö‡¶ó‡ßç‡¶®‡¶ø‡¶∏‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶æ‡¶∏ ‡¶ì ‡¶®‡ßà‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø ‡¶∏‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨...  \n...                                                  ...  \n35111  ‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡ßá‡¶∏‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡ßá‡¶∂‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡ßÄ ...  \n35112  ‡¶∞‡¶Æ‡¶ú‡¶æ‡¶® ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶∏‡¶ø‡¶≤‡ßá‡¶ü ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßÅ‡ßú‡¶ø‡¶∞ ‡¶Æ‡ßá‡ßü‡ßá ‡¶¨‡¶æ‡ßú‡¶ø ‡¶á‡¶´‡¶§‡¶æ‡¶∞‡¶ø...  \n35113  ‡¶¨‡¶ô‡ßç‡¶ó‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶∂‡ßá‡¶ñ ‡¶Æ‡ßÅ‡¶ú‡¶ø‡¶¨‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶æ‡¶Æ‡¶Ø‡ßá ‡¶®‡¶æ‡¶Æ...  \n35114  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶á ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∑‡ßç‡¶ü ‡¶®‡¶ø‡ßü‡ßá ‡¶ö‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá ‡¶∏‡ßá ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡¶§ ‡¶§‡¶æ‡¶ï...  \n35115  ‡¶∏‡ßá‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶è‡¶á ‡¶∂‡ßç‡¶≤‡ßã‡¶ó‡¶æ‡¶®‡ßá ‡¶∂‡¶§‡¶≠‡¶æ‡¶ó ‡¶Æ‡ßá‡¶ß‡¶æ‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶§...  \n\n[35110 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_article</th>\n      <th>clean_summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>‡¶§‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßÄ‡ßü ‡¶á‡¶ú‡¶Æ‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡ßÅ‡¶á‡¶°‡ßá‡¶®‡ßá‡¶∞ ...</td>\n      <td>‡¶§‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶æ‡¶û‡ßç‡¶ö‡¶≤‡ßÄ‡ßü ‡¶á‡¶ú‡¶Æ‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶¶‡ßá‡¶∂‡ßá ‡¶∏‡ßÅ‡¶á‡¶°‡ßá‡¶®‡ßá‡¶∞ ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶°‡ßá‡¶ô‡ßç‡¶ó‡ßÅ ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø ‡¶¶‡¶ø‡¶® ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶∞‡¶ì ‡¶≠‡ßü‡¶æ‡¶¨‡¶π ‡¶∞ ...</td>\n      <td>‡¶∏‡¶æ‡¶∞‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶°‡ßá‡¶ô‡ßç‡¶ó‡ßÅ ‡¶™‡¶∞‡¶ø‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø ‡¶¶‡¶ø‡¶® ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶∞‡¶ì ‡¶≠‡ßü‡¶æ‡¶¨‡¶π ‡¶∞ ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>‡¶∂‡ßã‡¶ï‡¶æ‡¶¨‡¶π ‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶¶‡¶ø‡¶®‡ßá ‡¶∏‡ßÅ‡¶®‡¶æ‡¶Æ‡¶ó‡¶û‡ßç‡¶ú‡ßá ‡¶∏‡ßç‡¶¨‡ßá‡¶ö‡ßç‡¶õ‡¶æ‡ßü...</td>\n      <td>‡¶∂‡ßã‡¶ï‡¶æ‡¶¨‡¶π ‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶¶‡¶ø‡¶®‡ßá ‡¶∏‡ßÅ‡¶®‡¶æ‡¶Æ‡¶ó‡¶û‡ßç‡¶ú‡ßá ‡¶∏‡ßç‡¶¨‡ßá‡¶ö‡ßç‡¶õ‡¶æ‡ßü...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>‡¶™‡¶æ‡¶®‡¶ø‡¶∏‡¶Æ‡ßç‡¶™‡¶¶ ‡¶â‡¶™‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶ï‡ßá‡¶è‡¶Æ ‡¶è‡¶®‡¶æ‡¶Æ‡ßÅ‡¶≤ ‡¶π‡¶ï ‡¶∂‡¶æ‡¶Æ‡ßÄ‡¶Æ ‡¶¨‡¶≤‡ßá‡¶õ...</td>\n      <td>‡¶™‡¶æ‡¶®‡¶ø‡¶∏‡¶Æ‡ßç‡¶™‡¶¶ ‡¶â‡¶™‡¶Æ‡¶®‡ßç‡¶§‡ßç‡¶∞‡ßÄ ‡¶è‡¶ï‡ßá‡¶è‡¶Æ ‡¶è‡¶®‡¶æ‡¶Æ‡ßÅ‡¶≤ ‡¶π‡¶ï ‡¶∂‡¶æ‡¶Æ‡ßÄ‡¶Æ ‡¶¨‡¶≤‡ßá‡¶õ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø‡¶∞ ‡¶Ö‡¶ó‡ßç‡¶®‡¶ø‡¶∏‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶æ‡¶∏ ‡¶ì ‡¶®‡ßà‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø ‡¶∏‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨...</td>\n      <td>‡¶¨‡¶ø‡¶è‡¶®‡¶™‡¶ø‡¶∞ ‡¶Ö‡¶ó‡ßç‡¶®‡¶ø‡¶∏‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶æ‡¶∏ ‡¶ì ‡¶®‡ßà‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø ‡¶∏‡ßÉ‡¶∑‡ßç‡¶ü‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>35111</th>\n      <td>‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡ßá‡¶∏‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡ßá‡¶∂‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡ßÄ ...</td>\n      <td>‡¶∞‡¶æ‡¶∂‡¶ø‡ßü‡¶æ‡ßü ‡¶™‡ßç‡¶∞‡ßá‡¶∏‡¶ø‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡ßá‡¶∂‡¶¨‡ßç‡¶Ø‡¶æ‡¶™‡ßÄ ...</td>\n    </tr>\n    <tr>\n      <th>35112</th>\n      <td>‡¶∞‡¶Æ‡¶ú‡¶æ‡¶® ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶∏‡¶ø‡¶≤‡ßá‡¶ü ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßÅ‡ßú‡¶ø‡¶∞ ‡¶Æ‡ßá‡ßü‡ßá ‡¶¨‡¶æ‡ßú‡¶ø ‡¶á‡¶´‡¶§‡¶æ‡¶∞‡¶ø...</td>\n      <td>‡¶∞‡¶Æ‡¶ú‡¶æ‡¶® ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶∏‡¶ø‡¶≤‡ßá‡¶ü ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßÅ‡ßú‡¶ø‡¶∞ ‡¶Æ‡ßá‡ßü‡ßá ‡¶¨‡¶æ‡ßú‡¶ø ‡¶á‡¶´‡¶§‡¶æ‡¶∞‡¶ø...</td>\n    </tr>\n    <tr>\n      <th>35113</th>\n      <td>‡¶¨‡¶ô‡ßç‡¶ó‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶∂‡ßá‡¶ñ ‡¶Æ‡ßÅ‡¶ú‡¶ø‡¶¨‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶æ‡¶Æ‡¶Ø‡ßá ‡¶®‡¶æ‡¶Æ...</td>\n      <td>‡¶¨‡¶ô‡ßç‡¶ó‡¶¨‡¶®‡ßç‡¶ß‡ßÅ ‡¶∂‡ßá‡¶ñ ‡¶Æ‡ßÅ‡¶ú‡¶ø‡¶¨‡ßÅ‡¶∞ ‡¶∞‡¶π‡¶Æ‡¶æ‡¶® ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶æ‡¶Æ‡¶Ø‡ßá ‡¶®‡¶æ‡¶Æ...</td>\n    </tr>\n    <tr>\n      <th>35114</th>\n      <td>‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶á ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∑‡ßç‡¶ü ‡¶®‡¶ø‡ßü‡ßá ‡¶ö‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá ‡¶∏‡ßá ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡¶§ ‡¶§‡¶æ‡¶ï...</td>\n      <td>‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶á ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∑‡ßç‡¶ü ‡¶®‡¶ø‡ßü‡ßá ‡¶ö‡¶≤‡ßá ‡¶ó‡ßá‡¶õ‡ßá ‡¶∏‡ßá ‡¶Æ‡¶®‡ßá ‡¶ï‡¶∞‡¶§ ‡¶§‡¶æ‡¶ï...</td>\n    </tr>\n    <tr>\n      <th>35115</th>\n      <td>‡¶∏‡ßá‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶è‡¶á ‡¶∂‡ßç‡¶≤‡ßã‡¶ó‡¶æ‡¶®‡ßá ‡¶∂‡¶§‡¶≠‡¶æ‡¶ó ‡¶Æ‡ßá‡¶ß‡¶æ‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶§...</td>\n      <td>‡¶∏‡ßá‡¶¨‡¶æ‡¶∞ ‡¶¨‡ßç‡¶∞‡¶§‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶è‡¶á ‡¶∂‡ßç‡¶≤‡ßã‡¶ó‡¶æ‡¶®‡ßá ‡¶∂‡¶§‡¶≠‡¶æ‡¶ó ‡¶Æ‡ßá‡¶ß‡¶æ‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø‡¶§...</td>\n    </tr>\n  </tbody>\n</table>\n<p>35110 rows √ó 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df.dropna(subset=['clean_article', 'clean_summary'])","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:12.938105Z","iopub.execute_input":"2024-10-02T11:34:12.938389Z","iopub.status.idle":"2024-10-02T11:34:12.958880Z","shell.execute_reply.started":"2024-10-02T11:34:12.938359Z","shell.execute_reply":"2024-10-02T11:34:12.958132Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.30, shuffle=True)\ndf_val, df_test = train_test_split(df_test, test_size=0.65,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:12.960119Z","iopub.execute_input":"2024-10-02T11:34:12.960433Z","iopub.status.idle":"2024-10-02T11:34:12.981943Z","shell.execute_reply.started":"2024-10-02T11:34:12.960402Z","shell.execute_reply":"2024-10-02T11:34:12.981257Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# from datasets import Dataset\n# ds_train = Dataset.from_pandas(df_train)\n# ds_test = Dataset.from_pandas(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:12.983077Z","iopub.execute_input":"2024-10-02T11:34:12.983453Z","iopub.status.idle":"2024-10-02T11:34:12.987529Z","shell.execute_reply.started":"2024-10-02T11:34:12.983404Z","shell.execute_reply":"2024-10-02T11:34:12.986649Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_train['clean_summary'][50]","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:12.988814Z","iopub.execute_input":"2024-10-02T11:34:12.989172Z","iopub.status.idle":"2024-10-02T11:34:13.000616Z","shell.execute_reply.started":"2024-10-02T11:34:12.989130Z","shell.execute_reply":"2024-10-02T11:34:12.999762Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'‡¶¨‡¶æ‡¶¨‡¶æ‡¶ï‡ßá ‡¶π‡¶æ‡¶®‡¶æ‡¶®‡ßã‡¶∞ ‡¶è‡¶ï ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π ‡¶™‡¶∞‡ßá‡¶á ‡¶Æ‡¶æ‡¶∞‡¶æ ‡¶ó‡ßá‡¶≤‡ßá‡¶® ‡¶π‡¶≤‡¶ø‡¶â‡¶° ‡¶Ö‡¶≠‡¶ø‡¶®‡ßá‡¶§‡¶æ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ô‡ßç‡¶ó‡¶æ‡¶∏ ‡¶ï‡ßç‡¶≤‡¶æ‡¶â‡¶° ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß®‡ß´ ‡¶¨‡¶õ‡¶∞ ‡¶¨‡ßü‡¶∏‡ßÄ ‡¶è ‡¶Ö‡¶≠‡¶ø‡¶®‡ßá‡¶§‡¶æ‡¶∞ ‡¶Æ‡ßÉ‡¶§‡ßç‡¶Ø‡ßÅ‡¶∞ ‡¶ñ‡¶¨‡¶∞‡¶ü‡¶ø ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶∞‡ßá ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶ó‡¶§ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶ú‡¶æ‡¶∞ ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü ‡¶¨‡ßá‡¶á‡¶≤‡¶ø ‡¶ó‡¶£‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡¶ï‡ßá ‡¶ú‡¶æ‡¶®‡¶ø‡ßü‡ßá‡¶õ‡ßá‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶â‡¶° ‡¶∏‡ßã‡¶Æ‡¶¨‡¶æ‡¶∞ ‡ß©‡ßß ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶´‡ßã‡¶∞‡ßç‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ì‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶¨‡¶æ‡¶°‡¶º‡¶ø‡¶§‡ßá ‡¶Æ‡¶æ‡¶∞‡¶æ ‡¶ó‡ßá‡¶õ‡ßá‡¶®'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq, Trainer, TrainingArguments\nimport os\n\n\n# Specify the model name for mT5\nmodel_name = \"google/mt5-base\"  # You can also use 'mt5-small', 'mt5-large', etc.\n\n# Load the mT5 model\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name)\n\n# Load the mT5 tokenizer\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:13.001768Z","iopub.execute_input":"2024-10-02T11:34:13.002532Z","iopub.status.idle":"2024-10-02T11:34:31.532832Z","shell.execute_reply.started":"2024-10-02T11:34:13.002492Z","shell.execute_reply":"2024-10-02T11:34:31.531791Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc68a416f51943ec9fe24fcca642d98d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a1a3e633be24f2492ee847974c3d557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96d86cf604145e38ea851976bb7a630"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b061746b0614be58ab4a9a1709cdfe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6a2cf533d6b4db9b493a1b5d35070b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc6a1ba097894d43abd624dd65c3beab"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:31.533894Z","iopub.execute_input":"2024-10-02T11:34:31.534191Z","iopub.status.idle":"2024-10-02T11:34:31.539874Z","shell.execute_reply.started":"2024-10-02T11:34:31.534160Z","shell.execute_reply":"2024-10-02T11:34:31.539026Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'google/mt5-base'"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass Seq2SeqDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        # Drop rows with NaN values in article or summary columns\n        data = data.dropna(subset=['clean_article', 'clean_summary'])\n\n        # Tokenize input text and labels (without normalization)\n        self.input_text = data['clean_article'].tolist()\n        self.labels = data['clean_summary'].tolist()\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.input_text)\n\n    def __getitem__(self, idx):\n        # Tokenize the input and labels\n        inputs = self.tokenizer(self.input_text[idx], \n                                max_length=self.max_length, \n                                padding='max_length', \n                                truncation=True, \n                                return_tensors=\"pt\")\n\n        labels = self.tokenizer(self.labels[idx], \n                                max_length=self.max_length, \n                                padding='max_length', \n                                truncation=True, \n                                return_tensors=\"pt\")\n\n        # Return input and labels for the sample\n        return {\n            'input_ids': inputs['input_ids'].squeeze(), \n            'attention_mask': inputs['attention_mask'].squeeze(), \n            'labels': labels['input_ids'].squeeze()\n        }\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:31.541027Z","iopub.execute_input":"2024-10-02T11:34:31.541353Z","iopub.status.idle":"2024-10-02T11:34:32.272424Z","shell.execute_reply.started":"2024-10-02T11:34:31.541322Z","shell.execute_reply":"2024-10-02T11:34:32.271283Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollatorForSeq2Seq\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass MyDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n    def __call__(self, features):\n        batch = {}\n\n        # Handle input_ids and attention_mask\n        batch[\"input_ids\"] = torch.stack([feature[\"input_ids\"] for feature in features])\n        batch[\"attention_mask\"] = torch.stack([feature[\"attention_mask\"] for feature in features])\n\n        # Check if the labels are PyTorch tensors\n        if isinstance(features[0][\"labels\"], torch.Tensor):\n            # Ensure padding for labels is handled correctly\n            batch[\"labels\"] = torch.stack([feature[\"labels\"] for feature in features])\n        else:\n            # Convert list of lists (if not tensor) to tensor and pad\n            batch[\"labels\"] = torch.tensor([feature[\"labels\"] for feature in features], dtype=torch.long)\n\n        # Make sure labels are padded if necessary (for variable-length sequences)\n        if self.label_pad_token_id is not None:\n            batch[\"labels\"] = pad_sequence([feature[\"labels\"] for feature in features], \n                                           batch_first=True, \n                                           padding_value=self.label_pad_token_id)\n\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:32.274283Z","iopub.execute_input":"2024-10-02T11:34:32.274832Z","iopub.status.idle":"2024-10-02T11:34:32.358755Z","shell.execute_reply.started":"2024-10-02T11:34:32.274778Z","shell.execute_reply":"2024-10-02T11:34:32.357583Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Create train, test, and validation datasets\ntrain_dataset = Seq2SeqDataset(df_train, tokenizer)\ntest_dataset = Seq2SeqDataset(df_test, tokenizer)\nvalidation_dataset = Seq2SeqDataset(df_val, tokenizer)\n\n# Create DataLoader for each dataset\ntrain_dataloader = DataLoader(train_dataset, \n                              batch_size=16, \n                              shuffle=True, \n                              collate_fn=MyDataCollatorForSeq2Seq(tokenizer))\n\ntest_dataloader = DataLoader(test_dataset, \n                             batch_size=16, \n                             collate_fn=MyDataCollatorForSeq2Seq(tokenizer))\n\nvalidation_dataloader = DataLoader(validation_dataset, \n                                   batch_size=16, \n                                   collate_fn=MyDataCollatorForSeq2Seq(tokenizer))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:32.360500Z","iopub.execute_input":"2024-10-02T11:34:32.360799Z","iopub.status.idle":"2024-10-02T11:34:32.479095Z","shell.execute_reply.started":"2024-10-02T11:34:32.360768Z","shell.execute_reply":"2024-10-02T11:34:32.478312Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Move the model to the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:32.482987Z","iopub.execute_input":"2024-10-02T11:34:32.483927Z","iopub.status.idle":"2024-10-02T11:34:32.627221Z","shell.execute_reply.started":"2024-10-02T11:34:32.483867Z","shell.execute_reply":"2024-10-02T11:34:32.626096Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:32.628860Z","iopub.execute_input":"2024-10-02T11:34:32.629304Z","iopub.status.idle":"2024-10-02T11:34:32.777347Z","shell.execute_reply.started":"2024-10-02T11:34:32.629244Z","shell.execute_reply":"2024-10-02T11:34:32.776166Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Create a custom optimizer using torch.optim.AdamW\ncustom_optimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=1e-3,\n    eps=1e-8,\n    weight_decay=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:32.778588Z","iopub.execute_input":"2024-10-02T11:34:32.778981Z","iopub.status.idle":"2024-10-02T11:34:33.320436Z","shell.execute_reply.started":"2024-10-02T11:34:32.778937Z","shell.execute_reply":"2024-10-02T11:34:33.319349Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define the TrainingArguments for fine-tuning\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/',\n    num_train_epochs=10,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    evaluation_strategy=\"epoch\",\n    save_total_limit=1,\n    save_steps=5000,\n    learning_rate=1e-3,\n    do_train=True,\n    do_eval=True,\n    remove_unused_columns=False,\n    push_to_hub=False,\n    report_to=\"none\",\n    load_best_model_at_end=False,\n    lr_scheduler_type=\"cosine_with_restarts\",\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_dir='/kaggle/working/',\n    logging_steps=200,\n    \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:33.321851Z","iopub.execute_input":"2024-10-02T11:34:33.322203Z","iopub.status.idle":"2024-10-02T11:34:33.378951Z","shell.execute_reply.started":"2024-10-02T11:34:33.322151Z","shell.execute_reply":"2024-10-02T11:34:33.377952Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# a7eadb1b11af8e041a6e1104b836b061cb0ff681\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:33.380073Z","iopub.execute_input":"2024-10-02T11:34:33.380370Z","iopub.status.idle":"2024-10-02T11:34:34.438051Z","shell.execute_reply.started":"2024-10-02T11:34:33.380339Z","shell.execute_reply":"2024-10-02T11:34:34.436638Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data_collator = MyDataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True,  # Padding to the model's max length\n    max_length=128,  # Use the same max length as during tokenization\n    label_pad_token_id=tokenizer.pad_token_id,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:34.439792Z","iopub.execute_input":"2024-10-02T11:34:34.440600Z","iopub.status.idle":"2024-10-02T11:34:34.500293Z","shell.execute_reply.started":"2024-10-02T11:34:34.440550Z","shell.execute_reply":"2024-10-02T11:34:34.499146Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from transformers.trainer import Trainer\n\nclass CustomTrainer(Trainer):\n    def _save(self, output_dir=None, state_dict=None):\n        # Ensure all tensors are contiguous before saving\n        for param in self.model.parameters():\n            if not param.is_contiguous():\n                param.data = param.data.contiguous()\n        \n        # Continue with normal save operation\n        super()._save(output_dir, state_dict)\n\n# Use CustomTrainer instead of Trainer\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    optimizers=(custom_optimizer, None),\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:34.501741Z","iopub.execute_input":"2024-10-02T11:34:34.502146Z","iopub.status.idle":"2024-10-02T11:34:35.501620Z","shell.execute_reply.started":"2024-10-02T11:34:34.502096Z","shell.execute_reply":"2024-10-02T11:34:35.500586Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T11:34:35.503296Z","iopub.execute_input":"2024-10-02T11:34:35.503707Z","iopub.status.idle":"2024-10-02T16:44:25.917050Z","shell.execute_reply.started":"2024-10-02T11:34:35.503661Z","shell.execute_reply":"2024-10-02T16:44:25.916131Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7680' max='7680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7680/7680 5:09:46, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.024200</td>\n      <td>0.015574</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.016600</td>\n      <td>0.011537</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.013600</td>\n      <td>0.010632</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.010800</td>\n      <td>0.010492</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.010500</td>\n      <td>0.010578</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.007800</td>\n      <td>0.011132</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.005700</td>\n      <td>0.010756</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.003500</td>\n      <td>0.012864</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.003000</td>\n      <td>0.013103</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7680, training_loss=0.10352895418570067, metrics={'train_runtime': 18589.3831, 'train_samples_per_second': 13.221, 'train_steps_per_second': 0.413, 'total_flos': 7.366130077099622e+16, 'train_loss': 0.10352895418570067, 'epoch': 9.998372660699756})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:44:25.918343Z","iopub.execute_input":"2024-10-02T16:44:25.918722Z","iopub.status.idle":"2024-10-02T16:44:42.355270Z","shell.execute_reply.started":"2024-10-02T16:44:25.918685Z","shell.execute_reply":"2024-10-02T16:44:42.354121Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=5c2d7174480b9dc50c727c43a7c17498832b7f53d40248f237f3990d359cde4a\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:44:42.356749Z","iopub.execute_input":"2024-10-02T16:44:42.357123Z","iopub.status.idle":"2024-10-02T16:44:55.601510Z","shell.execute_reply.started":"2024-10-02T16:44:42.357061Z","shell.execute_reply":"2024-10-02T16:44:55.600181Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.21.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install unidecode\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:44:55.602892Z","iopub.execute_input":"2024-10-02T16:44:55.603245Z","iopub.status.idle":"2024-10-02T16:45:08.738759Z","shell.execute_reply.started":"2024-10-02T16:44:55.603208Z","shell.execute_reply":"2024-10-02T16:45:08.737398Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting unidecode\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate ROUGE scores","metadata":{}},{"cell_type":"code","source":"import torch\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\n\n# Smoothing function for BLEU\nsmoothing_function = SmoothingFunction().method4\n\n# Define the move_to_device function\ndef move_to_device(batch, device):\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, list):\n        return [move_to_device(item, device) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: move_to_device(value, device) for key, value in batch.items()}\n    else:\n        return batch\n\n# Initialize lists to store generated summaries and references\ngenerated_summaries = []\nreferences = []\n\n# Generate summaries for the test dataset\nfor batch in test_dataloader:\n    # Move the batch to CUDA\n    batch = move_to_device(batch, 'cuda')\n\n    input_text = batch['input_ids']\n    labels = batch['labels']\n\n    # Generate summaries\n    with torch.no_grad():\n        summary_ids = model.generate(\n            input_text,\n            num_beams=6,\n            length_penalty=1.0,\n            early_stopping=True\n        )\n\n    # Move the summary_ids to CPU to decode\n    summary_ids = summary_ids.to('cpu')\n\n    # Decode generated summaries and labels\n    generated_summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n    reference_summary = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Extend lists with generated summaries and references\n    generated_summaries.extend(generated_summary)\n    references.extend(reference_summary)\n\n# Filter out empty predictions and references\nfiltered_generated_summaries = []\nfiltered_references = []\n\nfor pred, ref in zip(generated_summaries, references):\n    if pred.strip() and ref.strip():  # Check if both are non-empty\n        filtered_generated_summaries.append(pred)\n        filtered_references.append(ref)\n\n# Ensure the filtered lists are populated before calculating BLEU and ROUGE\nprint(f\"Filtered Generated Summaries Count: {len(filtered_generated_summaries)}\")\nprint(f\"Filtered References Count: {len(filtered_references)}\")\n\n# BLEU score calculation\ndef calculate_bleu(filtered_generated_summaries, filtered_references, tokenizer):\n    bleu_scores = []\n    for pred, ref in zip(filtered_generated_summaries, filtered_references):\n        pred_tokens = tokenizer.tokenize(pred)\n        ref_tokens = tokenizer.tokenize(ref)\n        if pred_tokens and ref_tokens:  # Ensure both are non-empty\n            score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothing_function)\n            bleu_scores.append(score)\n\n    avg_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n    return avg_bleu_score\n\n# Calculate BLEU\nbleu_score = calculate_bleu(filtered_generated_summaries, filtered_references, tokenizer)\nprint(f\"BLEU Score: {bleu_score}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:45:08.740800Z","iopub.execute_input":"2024-10-02T16:45:08.741551Z","iopub.status.idle":"2024-10-02T16:54:02.834357Z","shell.execute_reply.started":"2024-10-02T16:45:08.741498Z","shell.execute_reply":"2024-10-02T16:54:02.833425Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Filtered Generated Summaries Count: 6847\nFiltered References Count: 6847\nBLEU Score: 0.05685980408115119\n","output_type":"stream"}]},{"cell_type":"code","source":"from rouge_score import rouge_scorer\nfrom unidecode import unidecode\n\n# Initialize the Rouge scorer\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n# Define a function to preprocess and tokenize Bengali text\ndef preprocess_text(text):\n    text = unidecode(text)\n    tokens = text.split()\n    return ' '.join(tokens)\n\n# Create lists to store individual scores\nrouge1_f1_scores = []\nrouge1_precision_scores = []\nrouge1_recall_scores = []\nrouge2_f1_scores = []\nrouge2_precision_scores = []\nrouge2_recall_scores = []\nrougeL_f1_scores = []\nrougeL_precision_scores = []\nrougeL_recall_scores = []\n\nfor ref, pred in zip(references, generated_summaries):\n    candidate = preprocess_text(pred)\n    reference = preprocess_text(' '.join(ref))\n    scores = scorer.score(reference, candidate)\n\n    rouge1_f1_scores.append(scores['rouge1'].fmeasure)\n    rouge1_precision_scores.append(scores['rouge1'].precision)\n    rouge1_recall_scores.append(scores['rouge1'].recall)\n    rouge2_f1_scores.append(scores['rouge2'].fmeasure)\n    rouge2_precision_scores.append(scores['rouge2'].precision)\n    rouge2_recall_scores.append(scores['rouge2'].recall)\n    rougeL_f1_scores.append(scores['rougeL'].fmeasure)\n    rougeL_precision_scores.append(scores['rougeL'].precision)\n    rougeL_recall_scores.append(scores['rougeL'].recall)\n\n# Calculate the average scores\navg_rouge1_f1 = sum(rouge1_f1_scores) / len(rouge1_f1_scores)\navg_rouge1_precision = sum(rouge1_precision_scores) / len(rouge1_precision_scores)\navg_rouge1_recall = sum(rouge1_recall_scores) / len(rouge1_recall_scores)\navg_rouge2_f1 = sum(rouge2_f1_scores) / len(rouge2_f1_scores)\navg_rouge2_precision = sum(rouge2_precision_scores) / len(rouge2_precision_scores)\navg_rouge2_recall = sum(rouge2_recall_scores) / len(rouge2_recall_scores)\navg_rougeL_f1 = sum(rougeL_f1_scores) / len(rougeL_f1_scores)\navg_rougeL_precision = sum(rougeL_precision_scores) / len(rougeL_precision_scores)\navg_rougeL_recall = sum(rougeL_recall_scores) / len(rougeL_recall_scores)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:54:02.835498Z","iopub.execute_input":"2024-10-02T16:54:02.835827Z","iopub.status.idle":"2024-10-02T16:54:16.194160Z","shell.execute_reply.started":"2024-10-02T16:54:02.835793Z","shell.execute_reply":"2024-10-02T16:54:16.193075Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Print the average scores\nprint(\"Average Rouge-1 F1 Score:\", avg_rouge1_f1)\nprint(\"Average Rouge-1 Precision:\", avg_rouge1_precision)\nprint(\"Average Rouge-1 Recall:\", avg_rouge1_recall)\n\nprint(\"Average Rouge-2 F1 Score:\", avg_rouge2_f1)\nprint(\"Average Rouge-2 Precision:\", avg_rouge2_precision)\nprint(\"Average Rouge-2 Recall:\", avg_rouge2_recall)\n\nprint(\"Average Rouge-L F1 Score:\", avg_rougeL_f1)\nprint(\"Average Rouge-L Precision:\", avg_rougeL_precision)\nprint(\"Average Rouge-L Recall:\", avg_rougeL_recall)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:54:16.195480Z","iopub.execute_input":"2024-10-02T16:54:16.195788Z","iopub.status.idle":"2024-10-02T16:54:16.202332Z","shell.execute_reply.started":"2024-10-02T16:54:16.195755Z","shell.execute_reply":"2024-10-02T16:54:16.201308Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Average Rouge-1 F1 Score: 0.007826638760173693\nAverage Rouge-1 Precision: 0.0764327444296773\nAverage Rouge-1 Recall: 0.004157417230515951\nAverage Rouge-2 F1 Score: 0.0003857630018084146\nAverage Rouge-2 Precision: 0.003921362216966129\nAverage Rouge-2 Recall: 0.00020427769534275207\nAverage Rouge-L F1 Score: 0.007826638760173693\nAverage Rouge-L Precision: 0.0764327444296773\nAverage Rouge-L Recall: 0.004157417230515951\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_summaries","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n# import torch\n\n# # Assuming tokenized_df_test has been defined and is in the correct format\n# # Create DataLoader for the test set\n# sample_dataloader = DataLoader(\n#     tokenized_df_test,  # use the tokenized version\n#     collate_fn=data_collator,  # Ensure this collator is defined to handle padding\n#     batch_size=2\n# )\n\n# predictions = []\n# references = []\n\n# # Process the entire test set\n# for batch in sample_dataloader:\n#     with torch.no_grad():\n#         # Ensure input_ids and labels are moved to the correct device (e.g., GPU)\n#         input_ids = batch[\"input_ids\"].to(device)\n#         labels = batch[\"labels\"].to(device)\n\n#         # Generate predictions\n#         preds = model.generate(\n#             input_ids,\n#             num_beams=2,\n#             num_return_sequences=1,\n#             no_repeat_ngram_size=1,\n#             remove_invalid_values=True,\n#             max_length=128\n#         )\n    \n#     # Decode the predictions and labels\n#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     # Store the decoded predictions and labels\n#     predictions.extend(decoded_preds)\n#     references.extend(decoded_labels)\n\n# # Prepare the references for metric evaluation (references need to be wrapped in lists)\n# references = [[ref] for ref in references]\n\n# # Compute the metrics using already decoded text (BLEU/ROUGE)\n# metrics = metrics_func(predictions=predictions, references=references)\n# print(metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:54:16.203557Z","iopub.execute_input":"2024-10-02T16:54:16.203863Z","iopub.status.idle":"2024-10-02T16:54:16.214060Z","shell.execute_reply.started":"2024-10-02T16:54:16.203832Z","shell.execute_reply":"2024-10-02T16:54:16.213162Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# def metrics_func(eval_arg):\n#     preds, labels = eval_arg\n\n#     # Ensure the Bengali punctuation logic is applied\n#     text_preds = [(p if p.endswith((\"!\", \"?\", \"‡•§\")) else p + \"‡•§\") for p in preds]\n#     text_labels = [(l if l.endswith((\"!\", \"?\", \"‡•§\")) else l + \"‡•§\") for l in labels]\n\n#     # Tokenize sentences in Bengali\n#     sent_tokenizer_bn = RegexpTokenizer(u'[^!!??‡•§]*[!!??‡•§]')\n#     text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(p))) for p in text_preds]\n#     text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(l))) for l in text_labels]\n\n#     # Compute the ROUGE score directly from the text\n#     return rouge_metric.compute(predictions=text_preds, references=text_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:54:16.215212Z","iopub.execute_input":"2024-10-02T16:54:16.215698Z","iopub.status.idle":"2024-10-02T16:54:16.228888Z","shell.execute_reply.started":"2024-10-02T16:54:16.215660Z","shell.execute_reply":"2024-10-02T16:54:16.227988Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_metric\n\n# # Load ROUGE and/or BLEU metrics\n# rouge = load_metric(\"rouge\", trust_remote_code=True)\n# bleu = load_metric(\"bleu\")\n\n# def metrics_func(predictions, references):\n#     rouge_result = rouge.compute(predictions=predictions, references=references)\n#     bleu_result = bleu.compute(predictions=predictions, references=references)\n#     return {\"rouge\": rouge_result, \"bleu\": bleu_result}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:54:16.229991Z","iopub.execute_input":"2024-10-02T16:54:16.230340Z","iopub.status.idle":"2024-10-02T16:54:16.242683Z","shell.execute_reply.started":"2024-10-02T16:54:16.230307Z","shell.execute_reply":"2024-10-02T16:54:16.241846Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# # Compute the metrics using already decoded text (BLEU/ROUGE)\n# metrics = metrics_func(predictions=predictions, references=references)\n# print(metrics)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T16:54:16.243708Z","iopub.execute_input":"2024-10-02T16:54:16.243989Z","iopub.status.idle":"2024-10-02T16:54:16.251476Z","shell.execute_reply.started":"2024-10-02T16:54:16.243959Z","shell.execute_reply":"2024-10-02T16:54:16.250696Z"},"trusted":true},"execution_count":31,"outputs":[]}]}