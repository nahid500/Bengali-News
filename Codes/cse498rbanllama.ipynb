{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9548160,"sourceType":"datasetVersion","datasetId":5817419},{"sourceId":9676030,"sourceType":"datasetVersion","datasetId":5913714}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom transformers import MT5ForConditionalGeneration, AutoTokenizer ,DataCollatorForSeq2Seq, Trainer, TrainingArguments\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Trainer\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:00.744291Z","iopub.execute_input":"2025-01-05T04:47:00.744765Z","iopub.status.idle":"2025-01-05T04:47:07.273687Z","shell.execute_reply.started":"2025-01-05T04:47:00.744721Z","shell.execute_reply":"2025-01-05T04:47:07.272746Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install peft --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:07.274860Z","iopub.execute_input":"2025-01-05T04:47:07.275436Z","iopub.status.idle":"2025-01-05T04:47:10.505130Z","shell.execute_reply.started":"2025-01-05T04:47:07.275409Z","shell.execute_reply":"2025-01-05T04:47:10.504233Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:10.506888Z","iopub.execute_input":"2025-01-05T04:47:10.507122Z","iopub.status.idle":"2025-01-05T04:47:10.530142Z","shell.execute_reply.started":"2025-01-05T04:47:10.507101Z","shell.execute_reply":"2025-01-05T04:47:10.529174Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Remove nrows = ? for ","metadata":{}},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/cleaned-news-dataset/Cleaned_news_dataset.csv', usecols=['summary', 'article'], nrows=1000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cleaned-news-dataset/Cleaned_news_dataset.csv', usecols=['summary', 'article'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:10.531528Z","iopub.execute_input":"2025-01-05T04:47:10.531866Z","iopub.status.idle":"2025-01-05T04:47:13.408092Z","shell.execute_reply.started":"2025-01-05T04:47:10.531841Z","shell.execute_reply":"2025-01-05T04:47:13.407034Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:13.409105Z","iopub.execute_input":"2025-01-05T04:47:13.409459Z","iopub.status.idle":"2025-01-05T04:47:13.423822Z","shell.execute_reply.started":"2025-01-05T04:47:13.409424Z","shell.execute_reply":"2025-01-05T04:47:13.423130Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                 summary  \\\n0      তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...   \n1      সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...   \n2      শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...   \n3      পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...   \n4      বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...   \n...                                                  ...   \n37760  বঙ্গবন্ধু শেখ মুজিবুর রহমান এমন একটি নাম—যে না...   \n37761  আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...   \n37762  সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...   \n37763  দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...   \n37764  ‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...   \n\n                                                 article  \n0      তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...  \n1      সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...  \n2      শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...  \n3      পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...  \n4      বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...  \n...                                                  ...  \n37760  বঙ্গবন্ধু শেখ মুজিবুর রহমান এমন একটি নাম—যে না...  \n37761  আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...  \n37762  সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...  \n37763  দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...  \n37764  ‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...  \n\n[37765 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...</td>\n      <td>তুরস্কের পশ্চিমাঞ্চলীয় ইজমির প্রদেশে সুইডেনের ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...</td>\n      <td>সারাদেশে ডেঙ্গু পরিস্থিতি দিন দিন আরও ভয়াবহ রূ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...</td>\n      <td>শোকাবহ আগস্টের প্রথম দিনে সুনামগঞ্জে স্বেচ্ছায়...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...</td>\n      <td>পানিসম্পদ উপমন্ত্রী একেএম এনামুল হক শামীম বলেছ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...</td>\n      <td>বিএনপির ‌‘অগ্নিসন্ত্রাস ও নৈরাজ্য সৃষ্টির প্রত...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>37760</th>\n      <td>বঙ্গবন্ধু শেখ মুজিবুর রহমান এমন একটি নাম—যে না...</td>\n      <td>বঙ্গবন্ধু শেখ মুজিবুর রহমান এমন একটি নাম—যে না...</td>\n    </tr>\n    <tr>\n      <th>37761</th>\n      <td>আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...</td>\n      <td>আমার ভাই মনে কষ্ট নিয়ে চলে গেছে। সে মনে করত, ত...</td>\n    </tr>\n    <tr>\n      <th>37762</th>\n      <td>সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...</td>\n      <td>সারাজীবন আমি ভেবেছি সাদি আর শিবলী আমরা প্রায় স...</td>\n    </tr>\n    <tr>\n      <th>37763</th>\n      <td>দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...</td>\n      <td>দেশে শিশুশ্রম বেড়েছে। বাংলাদেশ পরিসংখ্যান ব্যু...</td>\n    </tr>\n    <tr>\n      <th>37764</th>\n      <td>‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...</td>\n      <td>‘সেবার ব্রতে চাকরি’ এই শ্লোগানে শতভাগ মেধা-যোগ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>37765 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.30, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:13.424790Z","iopub.execute_input":"2025-01-05T04:47:13.425109Z","iopub.status.idle":"2025-01-05T04:47:13.445432Z","shell.execute_reply.started":"2025-01-05T04:47:13.425078Z","shell.execute_reply":"2025-01-05T04:47:13.444866Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset\nds_train = Dataset.from_pandas(df_train)\nds_test = Dataset.from_pandas(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:13.446116Z","iopub.execute_input":"2025-01-05T04:47:13.446310Z","iopub.status.idle":"2025-01-05T04:47:15.515855Z","shell.execute_reply.started":"2025-01-05T04:47:13.446292Z","shell.execute_reply":"2025-01-05T04:47:15.514904Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Model on Alpaca-orca dataset","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-bangla-alpaca-orca-instruct-v0.0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-bangla-alpaca-orca-instruct-v0.0.1\")\n\n# Configure LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,   # Task type\n    inference_mode=False,           # Enable training mode\n    r=16,                           # LoRA rank\n    lora_alpha=32,                  # Scaling factor\n    lora_dropout=0.1                # Dropout rate\n)\n\n# Wrap the model with LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:47:15.517989Z","iopub.execute_input":"2025-01-05T04:47:15.518257Z","iopub.status.idle":"2025-01-05T04:49:04.051603Z","shell.execute_reply.started":"2025-01-05T04:47:15.518234Z","shell.execute_reply":"2025-01-05T04:49:04.050728Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5288782e024da6a53226e4add68f6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17fc00e69a64d7d92a8496c0e8a98cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24218f032891494fb0632a553a10e702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/917 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3bce8198a2e4dd2a0fd112ccdb3e504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b60ad3f59646d3bb69bdd913e1940d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/884 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4467f26e620e4af6b1f1e8b86003e984"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6bd552fe354030a58df49489210ef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779c527acb364cc4a3bc80ac56d8e5bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df8c8168760490fa5c52fff3fb78f58"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): ModulesToSaveWrapper(\n          (original_module): Embedding(128256, 2048)\n          (modules_to_save): ModuleDict(\n            (default): Embedding(128256, 2048)\n          )\n        )\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=2048, out_features=128256, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=2048, out_features=128256, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Model on culturax dataset","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-unolp-culturax-base-v0.0.1\")\nmodel = AutoModelForCausalLM.from_pretrained(\"BanglaLLM/BanglaLLama-3.2-1b-unolp-culturax-base-v0.0.1\")\n\n# Configure LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,   # Task type\n    inference_mode=False,           # Enable training mode\n    r=16,                           # LoRA rank\n    lora_alpha=32,                  # Scaling factor\n    lora_dropout=0.1                # Dropout rate\n)\n\n# Wrap the model with LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenize dataset with your model tokenizer","metadata":{}},{"cell_type":"code","source":"# Tokenize the data\ndef preprocess_function(examples):\n    inputs = tokenizer(\n        examples[\"article\"], max_length=256, truncation=True, padding=\"max_length\"\n    )\n    labels = tokenizer(\n        examples[\"summary\"], max_length=256, truncation=True, padding=\"max_length\"\n    )\n\n    # Replace pad token ID with -100 in labels (ignored during loss computation)\n    labels[\"input_ids\"] = [\n        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n        for label in labels[\"input_ids\"]\n    ]\n\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": labels[\"input_ids\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:49:04.052858Z","iopub.execute_input":"2025-01-05T04:49:04.053079Z","iopub.status.idle":"2025-01-05T04:49:04.057848Z","shell.execute_reply.started":"2025-01-05T04:49:04.053059Z","shell.execute_reply":"2025-01-05T04:49:04.057014Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\n\n\n\ntokenized_train = ds_train.map(\n    preprocess_function, batched=True, remove_columns=[\"summary\", \"article\", \"__index_level_0__\"]\n)\ntokenized_test = ds_test.map(\n    preprocess_function, batched=True, remove_columns=[\"summary\", \"article\", \"__index_level_0__\"]\n)\n\n\n\n# Define a data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"longest\",\n    return_tensors=\"pt\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:49:04.058547Z","iopub.execute_input":"2025-01-05T04:49:04.058823Z","iopub.status.idle":"2025-01-05T04:50:19.665025Z","shell.execute_reply.started":"2025-01-05T04:49:04.058800Z","shell.execute_reply":"2025-01-05T04:50:19.664109Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26435 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3935feccac0f464b80e374be9ab580bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2697b539d5d44379de74e8abc73b8a9"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Debug stuff","metadata":{}},{"cell_type":"code","source":"print(tokenized_train[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:50:19.665889Z","iopub.execute_input":"2025-01-05T04:50:19.666166Z","iopub.status.idle":"2025-01-05T04:50:19.672305Z","shell.execute_reply.started":"2025-01-05T04:50:19.666138Z","shell.execute_reply":"2025-01-05T04:50:19.671661Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [128000, 11372, 116, 50228, 105, 60008, 11372, 243, 36278, 116, 11372, 106, 50228, 250, 11372, 243, 11372, 110, 53906, 107, 50228, 96, 36278, 106, 87648, 53906, 97, 53906, 108, 28025, 222, 36278, 241, 36278, 110, 50228, 110, 11372, 106, 87648, 62456, 73358, 11372, 117, 50228, 253, 12, 28025, 101, 36278, 228, 11372, 116, 87648, 60008, 73358, 36278, 116, 11372, 224, 11372, 116, 11372, 99, 36278, 116, 11372, 99, 11372, 116, 53906, 107, 36278, 101, 28025, 223, 73358, 28025, 223, 11372, 250, 53906, 250, 50228, 106, 50228, 101, 36278, 228, 11372, 117, 11372, 106, 60008, 11372, 99, 60008, 73358, 36278, 249, 60008, 11372, 110, 60008, 36278, 108, 50228, 243, 81278, 105, 28025, 223, 11372, 250, 53906, 250, 50228, 106, 50228, 101, 36278, 228, 11372, 117, 11372, 106, 60008, 11372, 99, 36278, 248, 50228, 248, 50228, 108, 36278, 116, 11372, 247, 53906, 245, 60008, 36278, 103, 53906, 108, 11372, 97, 81278, 99, 53906, 105, 87648, 53906, 99, 53906, 105, 81278, 97, 42412, 36278, 243, 73358, 11372, 97, 60008, 36278, 231, 11372, 103, 11372, 250, 60008, 11372, 110, 42412, 36278, 103, 73358, 81278, 115, 11372, 99, 36278, 101, 62456, 73358, 53906, 105, 50228, 248, 87648, 60008, 36278, 106, 87648, 28025, 233, 87648, 42412, 28025, 253, 87648, 11372, 103, 11372, 97, 53906, 108, 36278, 250, 11372, 106, 42412, 36278, 99, 62456, 28025, 253, 60008, 11372, 249, 60008, 87648, 100278, 36278, 108, 28025, 233, 11372, 105, 11372, 105, 50228, 108, 320, 28025, 101, 28025, 100, 36278, 237, 11372, 103, 53906, 108, 81278, 110, 8, 36278, 110, 50228, 110, 11372, 106, 87648], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [128000, 11372, 116, 50228, 105, 60008, 11372, 243, 36278, 116, 11372, 106, 50228, 250, 11372, 243, 11372, 110, 53906, 107, 50228, 96, 36278, 106, 87648, 53906, 97, 53906, 108, 28025, 222, 36278, 241, 36278, 110, 50228, 110, 11372, 106, 87648, 62456, 73358, 11372, 117, 50228, 253, 12, 28025, 101, 36278, 228, 11372, 116, 87648, 60008, 73358, 36278, 116, 11372, 224, 11372, 116, 11372, 99, 36278, 116, 11372, 99, 11372, 116, 53906, 107, 36278, 101, 28025, 223, 73358, 28025, 223, 11372, 250, 53906, 250, 50228, 106, 50228, 101, 36278, 228, 11372, 117, 11372, 106, 60008, 11372, 99, 60008, 73358, 36278, 249, 60008, 11372, 110, 60008, 36278, 108, 50228, 243, 81278, 105, 28025, 223, 11372, 250, 53906, 250, 50228, 106, 50228, 101, 36278, 228, 11372, 117, 11372, 106, 60008, 11372, 99, 36278, 248, 50228, 248, 50228, 108, 36278, 116, 11372, 247, 53906, 245, 60008, 36278, 103, 53906, 108, 11372, 97, 81278, 99, 53906, 105, 87648, 53906, 99, 53906, 105, 81278, 97, 42412, 36278, 243, 73358, 11372, 97, 60008, 36278, 231, 11372, 103, 11372, 250, 60008, 11372, 110, 42412, 36278, 103, 73358, 81278, 115, 11372, 99, 36278, 101, 62456, 73358, 53906, 105, 50228, 248, 87648, 60008, 36278, 106, 87648, 28025, 233, 87648, 42412, 28025, 253, 87648, 11372, 103, 11372, 97, 53906, 108, 36278, 250, 11372, 106, 42412, 36278, 99, 62456, 28025, 253, 60008, 11372, 249, 60008, 87648, 100278, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"train_loader = DataLoader(tokenized_train, batch_size=2, collate_fn=data_collator)\nbatch = next(iter(train_loader))\nprint({key: val.shape for key, val in batch.items()})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:50:19.674327Z","iopub.execute_input":"2025-01-05T04:50:19.674534Z","iopub.status.idle":"2025-01-05T04:50:19.696470Z","shell.execute_reply.started":"2025-01-05T04:50:19.674512Z","shell.execute_reply":"2025-01-05T04:50:19.695650Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': torch.Size([2, 256]), 'attention_mask': torch.Size([2, 256]), 'labels': torch.Size([2, 256])}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"batch = {k: v.to(device) for k, v in batch.items()}\noutputs = model(**batch)\nprint(\"Loss:\", outputs.loss)\nprint(\"Logits shape:\", outputs.logits.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:50:19.697521Z","iopub.execute_input":"2025-01-05T04:50:19.697833Z","iopub.status.idle":"2025-01-05T04:50:20.915081Z","shell.execute_reply.started":"2025-01-05T04:50:19.697800Z","shell.execute_reply":"2025-01-05T04:50:20.914137Z"}},"outputs":[{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"name":"stdout","text":"Loss: tensor(1.0500, device='cuda:0', grad_fn=<NllLossBackward0>)\nLogits shape: torch.Size([2, 256, 128256])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\n\nbatch = next(iter(train_loader))\nbatch = {k: v.to(device) for k, v in batch.items()}\noutputs = model(**batch)\nprint(\"Logits:\", outputs.logits.shape)\n\n# Compute loss and backpropagate\nloss = outputs.loss\nloss.backward()\nprint(torch.cuda.memory_allocated(device) / 1024**3, \"GB used after backward pass\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:50:20.916155Z","iopub.execute_input":"2025-01-05T04:50:20.916495Z","iopub.status.idle":"2025-01-05T04:50:21.300414Z","shell.execute_reply.started":"2025-01-05T04:50:20.916459Z","shell.execute_reply":"2025-01-05T04:50:21.299487Z"}},"outputs":[{"name":"stdout","text":"Logits: torch.Size([2, 256, 128256])\n7.015652656555176 GB used after backward pass\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# ------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nprint(\"ran\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:50:21.301291Z","iopub.execute_input":"2025-01-05T04:50:21.301535Z","iopub.status.idle":"2025-01-05T04:50:21.306397Z","shell.execute_reply.started":"2025-01-05T04:50:21.301512Z","shell.execute_reply":"2025-01-05T04:50:21.305514Z"}},"outputs":[{"name":"stdout","text":"ran\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",          # Save model here\n    eval_strategy=\"epoch\",    # Evaluate every epoch\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    predict_with_generate=False,     # Generate summaries during eval\n    logging_dir='./logs',           # For logging\n    logging_steps=100,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    fp16 = True,\n    dataloader_num_workers=2,        # Optimize data loading\n    gradient_accumulation_steps=4,   # Adjust if batch size is small\n    \n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T04:50:21.307220Z","iopub.execute_input":"2025-01-05T04:50:21.307433Z","iopub.status.idle":"2025-01-05T13:15:15.438632Z","shell.execute_reply.started":"2025-01-05T04:50:21.307412Z","shell.execute_reply":"2025-01-05T13:15:15.437729Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4956' max='4956' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4956/4956 8:24:47, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.727800</td>\n      <td>0.722407</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.701400</td>\n      <td>0.704902</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.683200</td>\n      <td>0.700519</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4956, training_loss=0.7261932440781612, metrics={'train_runtime': 30293.0486, 'train_samples_per_second': 2.618, 'train_steps_per_second': 0.164, 'total_flos': 1.5305501109859123e+17, 'train_loss': 0.7261932440781612, 'epoch': 2.999546073536087})"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Save model?","metadata":{}},{"cell_type":"code","source":"# Save the model and tokenizer\ntrainer.save_model(\"/kaggle/working/saved_model\")  # Save in Kaggle working directory\ntokenizer.save_pretrained(\"/kaggle/working/saved_model\")  # Save tokenizer files\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T13:46:07.201594Z","iopub.execute_input":"2025-01-05T13:46:07.202011Z","iopub.status.idle":"2025-01-05T13:46:08.243081Z","shell.execute_reply.started":"2025-01-05T13:46:07.201980Z","shell.execute_reply":"2025-01-05T13:46:08.242090Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/saved_model/tokenizer_config.json',\n '/kaggle/working/saved_model/special_tokens_map.json',\n '/kaggle/working/saved_model/tokenizer.json')"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Compress the saved model directory\nshutil.make_archive(\"/kaggle/working/saved_model\", 'zip', \"/kaggle/working/saved_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T13:46:10.820866Z","iopub.execute_input":"2025-01-05T13:46:10.821512Z","iopub.status.idle":"2025-01-05T13:46:15.296345Z","shell.execute_reply.started":"2025-01-05T13:46:10.821476Z","shell.execute_reply":"2025-01-05T13:46:15.295472Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/saved_model.zip'"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# Get ROUGE scores","metadata":{}},{"cell_type":"code","source":"pip install rouge_score evaluate --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T13:15:15.440442Z","iopub.execute_input":"2025-01-05T13:15:15.440704Z","iopub.status.idle":"2025-01-05T13:15:20.834658Z","shell.execute_reply.started":"2025-01-05T13:15:15.440679Z","shell.execute_reply":"2025-01-05T13:15:20.833576Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nfrom nltk.tokenize import RegexpTokenizer\n\nrouge_metric = evaluate.load(\"rouge\")\n\n# define function for custom tokenization\ndef tokenize_sentence(arg):\n    encoded_arg = tokenizer(arg)\n    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n\n# define function to get ROUGE scores with custom tokenization\ndef metrics_func(eval_arg):\n    preds, labels = eval_arg\n\n    # Replace -100\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Convert id tokens to text\n\n    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Insert a line break (\\n) in each sentence for ROUGE scoring\n\n    # (Note : Please change this code, when you perform on other languages except for Bengali)\n    text_preds = [(p if p.endswith((\"!\", \"!\", \"?\", \"?\", \"।\")) else p + \"।\") for p in text_preds]\n    text_labels = [(l if l.endswith((\"!\", \"!\", \"?\", \"?\", \"।\")) else l + \"।\") for l in text_labels]\n    sent_tokenizer_bn = RegexpTokenizer(u'[^!!??।]*[!!??।]')\n    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(p))) for p in text_preds]\n    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_bn.tokenize(l))) for l in text_labels]\n\n    # compute ROUGE score with custom tokenization\n    return rouge_metric.compute(\n    predictions=text_preds,\n    references=text_labels,\n    tokenizer=tokenize_sentence\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T13:15:20.836542Z","iopub.execute_input":"2025-01-05T13:15:20.836882Z","iopub.status.idle":"2025-01-05T13:15:22.510085Z","shell.execute_reply.started":"2025-01-05T13:15:20.836847Z","shell.execute_reply":"2025-01-05T13:15:22.509195Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08712508f87840cb99d72d621e9034fa"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Dataloader takes data in random sequence randomly, so results may vary","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntesting_dataloader = DataLoader(\n    tokenized_test.with_format(\"torch\"),\n    collate_fn=data_collator,\n    batch_size=1\n)\n\nfor batch in testing_dataloader:\n    with torch.no_grad():\n        preds = model.generate(\n            batch[\"input_ids\"].to(device),\n            num_beams=2,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            remove_invalid_values=True,\n            max_new_tokens=256\n        )\n    labels = batch[\"labels\"]\n    break\n\nmetrics_func([preds, labels])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-05T13:15:22.511149Z","iopub.execute_input":"2025-01-05T13:15:22.511502Z","iopub.status.idle":"2025-01-05T13:15:25.637757Z","shell.execute_reply.started":"2025-01-05T13:15:22.511465Z","shell.execute_reply":"2025-01-05T13:15:25.637003Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.9162210338680927,\n 'rouge2': 0.9123434704830053,\n 'rougeL': 0.9162210338680927,\n 'rougeLsum': 0.9168141592920354}"},"metadata":{}}],"execution_count":19}]}